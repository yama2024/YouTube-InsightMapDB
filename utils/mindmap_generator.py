import logging
from typing import Dict, List, Tuple
import json

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MindMapGenerator:
    def __init__(self):
        self._cache = {}

    def _create_mermaid_mindmap(self, data: Dict) -> str:
        """Generate Mermaid mindmap syntax with proper formatting for version 10.2.4"""
        try:
            logger.debug(f"Starting mindmap creation with data structure: {list(data.keys())}")
            lines = ["mindmap"]
            
            # Root node with proper Mermaid 10.2.4 syntax
            overview = self._clean_text(data.get("å‹•ç”»ã®æ¦‚è¦", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¦‚è¦"))
            MAX_OVERVIEW_LENGTH = 50  # Reduced for better display
            if len(overview) > MAX_OVERVIEW_LENGTH:
                overview = overview[:MAX_OVERVIEW_LENGTH-3] + "..."
            lines.append(f"  root((æ¦‚è¦ï¼š{overview}))")
            logger.debug(f"Added root node: {overview}")
            
            # Process points with enhanced validation
            points = data.get("ãƒã‚¤ãƒ³ãƒˆ", [])
            if not points or not isinstance(points, list):
                logger.warning("Invalid or empty points structure")
                return self._create_fallback_mindmap()
            
            logger.debug(f"Processing {len(points)} points")
            
            # Group points by importance for better organization
            points_by_importance = {i: [] for i in range(1, 6)}
            for point in points:
                if isinstance(point, dict):
                    importance = min(max(int(point.get("é‡è¦åº¦", 3)), 1), 5)
                    points_by_importance[importance].append(point)
            
            # Process points by importance (high to low)
            point_counter = 1
            for importance in reversed(range(1, 6)):
                for point in points_by_importance[importance]:
                    title = self._clean_text(point.get("ã‚¿ã‚¤ãƒˆãƒ«", ""))
                    if not title or len(title.strip()) < 2:
                        logger.warning(f"Skipping point {point_counter} due to invalid title")
                        continue
                    
                    # Enhanced importance indicators
                    importance_marks = {
                        5: "ğŸ”¥ é‡è¦",
                        4: "â­ æ³¨ç›®",
                        3: "ğŸ“Œ",
                        2: "ãƒ»",
                        1: "âˆ™"
                    }
                    importance_mark = importance_marks[importance]
                    
                    # Add formatted title node with consistent indentation
                    title_node = f"  {point_counter}[{importance_mark} {title}]"
                    lines.append(title_node)
                    
                    # Process content with improved chunking
                    content = self._clean_text(point.get("å†…å®¹", ""))
                    if content:
                        # Optimized content chunking
                        chunk_size = 40  # Consistent chunk size
                        content_parts = []
                        current_part = ""
                        
                        # Split by Japanese sentence endings for more natural breaks
                        sentences = [s.strip() for s in re.split('[ã€‚ï¼ï¼ï¼Ÿ]', content) if s.strip()]
                        for sentence in sentences:
                            if len(sentence) <= chunk_size:
                                content_parts.append(sentence)
                            else:
                                # Split long sentences
                                words = sentence.split()
                                for word in words:
                                    if len(current_part) + len(word) + 1 <= chunk_size:
                                        current_part += f" {word}" if current_part else word
                                    else:
                                        if current_part:
                                            content_parts.append(current_part.strip())
                                        current_part = word
                                if current_part:
                                    content_parts.append(current_part.strip())
                                    current_part = ""
                        
                        # Add content nodes with proper indentation
                        for j, part in enumerate(content_parts, 1):
                            if part.strip():
                                content_node = f"    {point_counter}.{j}[{part}]"
                                lines.append(content_node)
                    
                    # Enhanced supplementary info handling
                    if "è£œè¶³æƒ…å ±" in point and point["è£œè¶³æƒ…å ±"]:
                        suppl_info = self._clean_text(point["è£œè¶³æƒ…å ±"])
                        if suppl_info:
                            lines.append(f"      {point_counter}.s[ğŸ’¡ {suppl_info[:45]}...]")
                    
                    point_counter += 1
            
            # Conclusion formatting with proper Mermaid syntax
            conclusion = self._clean_text(data.get("çµè«–", ""))
            if conclusion:
                lines.append("  c[çµè«–]")
                # Split conclusion into meaningful chunks
                sentences = [s.strip() for s in re.split('[ã€‚ï¼ï¼ï¼Ÿ]', conclusion) if s.strip()]
                for i, sentence in enumerate(sentences, 1):
                    if len(sentence) > 40:
                        # Split long sentences into smaller chunks
                        chunks = []
                        current_chunk = ""
                        words = sentence.split()
                        for word in words:
                            if len(current_chunk) + len(word) + 1 <= 40:
                                current_chunk += f" {word}" if current_chunk else word
                            else:
                                if current_chunk:
                                    chunks.append(current_chunk.strip())
                                current_chunk = word
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        
                        # Add chunked sentences
                        for j, chunk in enumerate(chunks, 1):
                            lines.append(f"    c.{i}.{j}[{chunk}]")
                    else:
                        # Add short sentences directly
                        lines.append(f"    c.{i}[{sentence}]")
            
            mindmap = "\n".join(lines)
            logger.debug(f"Generated mindmap structure:\n{mindmap}")
            return mindmap
            
        except Exception as e:
            logger.error(f"Error in mindmap creation: {str(e)}", exc_info=True)
            return self._create_fallback_mindmap()

    def _clean_text(self, text: str) -> str:
        """Clean and normalize text for Mermaid 10.2.4 syntax with strict character handling"""
        try:
            # Input validation
            if not isinstance(text, str):
                text = str(text)
            if not text.strip():
                return "å†…å®¹ãªã—"
                
            # Length validation
            MAX_LENGTH = 150  # Reduced for better display
            if len(text) > MAX_LENGTH:
                text = text[:MAX_LENGTH-3] + "..."
            
            # Basic normalization
            text = text.strip()
            text = " ".join(text.split())  # Normalize whitespace
            
            # Mermaid syntax characters that need escaping
            replacements = {
                '"': "'",    # Replace quotes
                '\n': " ",   # Replace newlines
                '[': "ï¼»",   # Safe brackets
                ']': "ï¼½",
                '(': "ï¼ˆ",   # Safe parentheses
                ')': "ï¼‰",
                '<': "ï¼œ",   # Safe angle brackets
                '>': "ï¼",
                '|': "ï½œ",   # Safe vertical bar
                '*': "ï¼Š",   # Safe asterisk
                '#': "ï¼ƒ",   # Safe hash
                '^': "ï¼¾",   # Safe caret
                '~': "ï½",   # Safe tilde
                '`': "ï½€",   # Safe backtick
                ':': "ï¼š",   # Safe colon
                ';': "ï¼›",   # Safe semicolon
                '&': "ï¼†",   # Safe ampersand
                '%': "ï¼…",   # Safe percent
                '$': "ï¼„",   # Safe dollar
                '@': "ï¼ ",   # Safe at
                '!': "ï¼",   # Safe exclamation
                '?': "ï¼Ÿ",   # Safe question mark
                '+': "ï¼‹",   # Safe plus
                '=': "ï¼",   # Safe equals
                '{': "ï½›",   # Safe braces
                '}': "ï½",
            }
            
            # Apply replacements while preserving important formatting
            for old, new in replacements.items():
                text = text.replace(old, new)
            
            # Preserve important formatting markers
            preserved_markers = ['!', 'ï¼Ÿ', 'ï¼', 'ã€‚', 'ã€']
            for marker in preserved_markers:
                text = text.replace(f"{marker} ", marker)
            
            # Final cleanup
            text = text.strip()
            
            # Validate final length
            if not text:
                return "å†…å®¹ãªã—"
            
            return text
            
        except Exception as e:
            logger.error(f"Text cleaning error: {str(e)}")
            return "ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼"

    def _create_fallback_mindmap(self) -> str:
        """Create a more informative fallback mindmap when generation fails"""
        return """mindmap
  root[ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æçµæœ]
    1[âš ï¸ å‡¦ç†çŠ¶æ…‹]
      1.1[ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ã®ç”Ÿæˆã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ]
      1.2[ä»¥ä¸‹ã‚’ã”ç¢ºèªãã ã•ã„]
        1.2.1[ãƒ»å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼]
        1.2.2[ãƒ»ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•]
        1.2.3[ãƒ»ç‰¹æ®Šæ–‡å­—ã®ä½¿ç”¨]
    2[ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—]
      2.1[ãƒ»ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°]
      2.2[ãƒ»å…¥åŠ›ã‚’ç¢ºèª]
      2.3[ãƒ»å†åº¦å®Ÿè¡Œ]"""

    def _validate_json_structure(self, data: Dict) -> bool:
        """Validate the JSON structure with strict validation, quality checks, and detailed error messages"""
        try:
            # Enhanced logging
            logger.debug(f"Starting JSON validation for data type: {type(data)}")
            logger.debug(f"Available keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
            
            # Strict type validation
            if not isinstance(data, dict):
                logger.error(f"ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒç„¡åŠ¹ã§ã™ã€‚è¾æ›¸å‹ãŒå¿…è¦ã§ã™ãŒã€{type(data)}ãŒä¸ãˆã‚‰ã‚Œã¾ã—ãŸã€‚")
                return False
            
            # Required keys validation with enhanced content checks
            validation_rules = {
                "å‹•ç”»ã®æ¦‚è¦": {
                    "type": str,
                    "min_length": 20,
                    "max_length": 500,
                    "required_chars": ["ã€‚", "ã€"],
                    "error_prefix": "å‹•ç”»æ¦‚è¦"
                },
                "ãƒã‚¤ãƒ³ãƒˆ": {
                    "type": list,
                    "min_items": 1,
                    "max_items": 10,
                    "error_prefix": "ãƒã‚¤ãƒ³ãƒˆé…åˆ—"
                },
                "çµè«–": {
                    "type": str,
                    "min_length": 15,
                    "max_length": 300,
                    "required_chars": ["ã€‚"],
                    "error_prefix": "çµè«–"
                }
            }
            
            # Validate each required key
            for key, rules in validation_rules.items():
                # Check existence
                if key not in data:
                    logger.error(f"{rules['error_prefix']}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return False
                
                value = data[key]
                
                # Type validation
                if not isinstance(value, rules["type"]):
                    logger.error(
                        f"{rules['error_prefix']}ã®å‹ãŒç„¡åŠ¹ã§ã™: "
                        f"{rules['type'].__name__}ãŒå¿…è¦ã§ã™ãŒã€{type(value).__name__}ãŒä¸ãˆã‚‰ã‚Œã¾ã—ãŸ"
                    )
                    return False
                
                # Content validation for strings
                if rules["type"] == str:
                    cleaned_value = value.strip()
                    if len(cleaned_value) < rules["min_length"]:
                        logger.error(
                            f"{rules['error_prefix']}ãŒçŸ­ã™ãã¾ã™: "
                            f"æœ€å°{rules['min_length']}æ–‡å­—å¿…è¦ã§ã™ãŒã€{len(cleaned_value)}æ–‡å­—ã§ã™"
                        )
                        return False
                    if len(cleaned_value) > rules["max_length"]:
                        logger.error(
                            f"{rules['error_prefix']}ãŒé•·ã™ãã¾ã™: "
                            f"æœ€å¤§{rules['max_length']}æ–‡å­—ã¾ã§ã§ã™ãŒã€{len(cleaned_value)}æ–‡å­—ã§ã™"
                        )
                        return False
                    
                    # Check for required characters
                    if "required_chars" in rules:
                        missing_chars = [c for c in rules["required_chars"] if c not in cleaned_value]
                        if missing_chars:
                            logger.error(
                                f"{rules['error_prefix']}ã«å¿…è¦ãªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“: "
                                f"{', '.join(missing_chars)}"
                            )
                            return False
                
                # Content validation for lists
                if rules["type"] == list:
                    if len(value) < rules["min_items"]:
                        logger.error(
                            f"{rules['error_prefix']}ã®é …ç›®æ•°ãŒå°‘ãªã™ãã¾ã™: "
                            f"æœ€å°{rules['min_items']}é …ç›®å¿…è¦ã§ã™ãŒã€{len(value)}é …ç›®ã§ã™"
                        )
                        return False
                    if len(value) > rules["max_items"]:
                        logger.error(
                            f"{rules['error_prefix']}ã®é …ç›®æ•°ãŒå¤šã™ãã¾ã™: "
                            f"æœ€å¤§{rules['max_items']}é …ç›®ã¾ã§ã§ã™ãŒã€{len(value)}é …ç›®ã§ã™"
                        )
                        return False
            
            # Validate points array
            points = data.get("ãƒã‚¤ãƒ³ãƒˆ", [])
            if not isinstance(points, list) or not points:
                logger.error(f"Points must be non-empty list, got {type(points)}")
                return False
            
            # Validate each point structure
            for i, point in enumerate(points):
                if not self._validate_point_structure(point, i):
                    return False
            
            # Additional validation for nested structures
            if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in data:
                keywords = data["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]
                if not isinstance(keywords, list):
                    logger.error("Keywords must be a list")
                    return False
                for i, keyword in enumerate(keywords):
                    if not isinstance(keyword, dict) or "ç”¨èª" not in keyword or "èª¬æ˜" not in keyword:
                        logger.error(f"Invalid keyword structure at index {i}")
                        return False
            
            logger.info("JSON structure validation successful")
            logger.debug("All validations passed successfully")
            return True
            
        except Exception as e:
            logger.error(f"Validation error: {str(e)}", exc_info=True)
            return False
            
    def _validate_point_structure(self, point: Dict, index: int) -> bool:
        """Validate individual point structure with improved null handling"""
        if not isinstance(point, dict):
            logger.error(f"Point {index} is not a dictionary")
            return False
            
        # Required fields validation
        required_fields = {
            "ã‚¿ã‚¤ãƒˆãƒ«": (str, 1, 100),  # (type, min_length, max_length)
            "å†…å®¹": (str, 10, 1000),
            "é‡è¦åº¦": (int, 1, 5)
        }
        
        for field, (expected_type, min_val, max_val) in required_fields.items():
            value = point.get(field)
            
            # Handle missing or None values
            if value is None:
                logger.error(f"Point {index} field {field} is missing or None")
                return False
                
            # Type validation
            if not isinstance(value, expected_type):
                try:
                    # Try type conversion for numbers
                    if expected_type == int:
                        value = int(value)
                        point[field] = value
                    else:
                        logger.error(f"Point {index} field {field} has wrong type: expected {expected_type}, got {type(value)}")
                        return False
                except (ValueError, TypeError):
                    logger.error(f"Point {index} field {field} cannot be converted to {expected_type}")
                    return False
                
            # Length/range validation
            if expected_type == str:
                cleaned_value = value.strip()
                if not cleaned_value or len(cleaned_value) < min_val or len(cleaned_value) > max_val:
                    logger.error(f"Point {index} field {field} length out of range: {len(cleaned_value)} not in [{min_val}, {max_val}]")
                    return False
                    
            if expected_type == int and (value < min_val or value > max_val):
                logger.error(f"Point {index} field {field} value out of range: {value} not in [{min_val}, {max_val}]")
                return False
        
        return True
            
    def generate_mindmap(self, text: str) -> Tuple[str, bool]:
        """Generate a mindmap from the analyzed text with enhanced validation"""
        try:
            if not text or not isinstance(text, str):
                logger.error(f"Invalid input text type: {type(text)}")
                return self._create_fallback_mindmap(), False

            logger.info(f"Generating mindmap for text of length: {len(text)}")
            
            # Parse JSON text and validate structure
            try:
                data = json.loads(text)
                logger.debug(f"Parsed JSON data structure: {list(data.keys())}")
                
                # Validate JSON structure
                if not self._validate_json_structure(data):
                    logger.error("Invalid JSON structure")
                    return self._create_fallback_mindmap(), False
                    
                # Generate mindmap
                mindmap = self._create_mermaid_mindmap(data)
                return mindmap, True
                    
            except json.JSONDecodeError as e:
                logger.error(f"JSON parsing error: {str(e)}")
                return self._create_fallback_mindmap(), False
            except Exception as e:
                logger.error(f"Mindmap generation error: {str(e)}")
                return self._create_fallback_mindmap(), False
            cache_key = hash(f"{text}_{self.__class__.__name__}")
            if cache_key in self._cache:
                logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ã‚’å–å¾—ã—ã¾ã—ãŸ")
                cached_content = self._cache[cache_key]
                logger.debug(f"Cached mindmap length: {len(cached_content)}")
                return cached_content, True

            # Parse and validate JSON
            try:
                logger.debug("Attempting to parse JSON data")
                data = json.loads(text)
                logger.info("JSON parsing successful")
                
                if not self._validate_json_structure(data):
                    logger.warning("Invalid JSON structure detected, using fallback structure")
                    logger.debug(f"Received data structure: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
                    data = {
                        "å‹•ç”»ã®æ¦‚è¦": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¦ç´„",
                        "ãƒã‚¤ãƒ³ãƒˆ": [
                            {
                                "ã‚¿ã‚¤ãƒˆãƒ«": "ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆ",
                                "å†…å®¹": "å‹•ç”»ã®å†…å®¹ã‚’ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸ",
                                "é‡è¦åº¦": 3
                            }
                        ],
                        "çµè«–": "å†…å®¹ã‚’ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸ"
                    }
                    logger.info("Using fallback data structure")
            except json.JSONDecodeError as e:
                logger.warning("Invalid JSON format, creating basic structure")
                data = {
                    "å‹•ç”»ã®æ¦‚è¦": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¦‚è¦",
                    "ãƒã‚¤ãƒ³ãƒˆ": [{
                        "ã‚¿ã‚¤ãƒˆãƒ«": "æ¦‚è¦",
                        "å†…å®¹": text[:100] + "..." if len(text) > 100 else text,
                        "é‡è¦åº¦": 3
                    }],
                    "çµè«–": "ãƒ†ã‚­ã‚¹ãƒˆã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ"
                }

            # Generate mindmap with validated data
            mermaid_syntax = self._create_mermaid_mindmap(data)
            
            # Cache only valid results
            if mermaid_syntax and mermaid_syntax.count('\n') > 2:
                self._cache[cache_key] = mermaid_syntax
                logger.info("æ–°ã—ã„ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ")
                return mermaid_syntax, True
            
            logger.warning("ç”Ÿæˆã•ã‚ŒãŸãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ãŒç„¡åŠ¹ã§ã™")
            return self._create_fallback_mindmap(), False
            
        except Exception as e:
            logger.error(f"ãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return self._create_fallback_mindmap(), False

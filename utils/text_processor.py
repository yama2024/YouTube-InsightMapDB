import os
import logging
import time
from time import sleep
from random import uniform
import hashlib
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
import re
from cachetools import TTLCache
from typing import Optional, Callable, List, Dict, Any
import json
import google.generativeai as genai

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DynamicRateLimiter:
    def __init__(self):
        self.last_request = 0
        self.min_interval = 3.0  # ã‚ˆã‚Šé•·ã„æœ€å°å¾…æ©Ÿæ™‚é–“
        self.backoff_multiplier = 1.0
        self.max_interval = 15.0  # ã‚ˆã‚Šé•·ã„æœ€å¤§å¾…æ©Ÿæ™‚é–“
        self.success_count = 0
        self.error_count = 0
        self.quota_exceeded = False
        self.request_times = []
        self.window_size = 60  # 1åˆ†é–“ã®æ™‚é–“æ 
        self.max_requests = 30  # 1åˆ†é–“ã‚ãŸã‚Šã®æœ€å¤§ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’åˆ¶é™

    def wait(self):
        now = time.time()
        
        # æ™‚é–“æ å†…ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ›´æ–°
        self.request_times = [t for t in self.request_times if now - t < self.window_size]
        
        if len(self.request_times) >= self.max_requests:
            sleep_time = self.window_size - (now - self.request_times[0])
            if sleep_time > 0:
                time.sleep(sleep_time)
        
        if self.quota_exceeded:
            time.sleep(self.max_interval)
            self.quota_exceeded = False
            return
        
        wait_time = max(0, self.min_interval * self.backoff_multiplier)
        if wait_time > 0:
            time.sleep(wait_time)
        
        self.request_times.append(now)
        self.last_request = now

    def report_success(self):
        self.success_count += 1
        self.backoff_multiplier = max(1.0, self.backoff_multiplier * 0.95)

    def report_error(self):
        self.error_count += 1
        self.backoff_multiplier = min(self.max_interval, self.backoff_multiplier * 1.5)

    def report_quota_exceeded(self):
        self.quota_exceeded = True
        self.error_count += 1
        self.backoff_multiplier = min(self.max_interval, self.backoff_multiplier * 2.0)

class TextProcessor:
    def __init__(self):
        self._initialize_api()
        self.rate_limiter = DynamicRateLimiter()
        self.cache = TTLCache(maxsize=100, ttl=3600)
        self.chunk_size = 2500
        self.overlap_size = 200
        self.max_retries = 3  # Updated to match new implementation
        self.backoff_factor = 2
        self.context_memory = []
        self.max_context_memory = 5
        self.summary_cache = {}

    def _initialize_api(self):
        """Initialize or reinitialize the Gemini API with current environment"""
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("Gemini API key is not set in environment variables")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-pro')

    def _split_text_into_chunks(self, text: str) -> List[str]:
        if len(text) < self.chunk_size:
            return [text]
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        # æ–‡å˜ä½ã§ã®åˆ†å‰²ï¼ˆæ”¹è¡Œã‚‚è€ƒæ…®ï¼‰
        sentences = [s.strip() for s in re.split('[ã€‚.!?ï¼ï¼Ÿ\n]', text) if s.strip()]
        
        for sentence in sentences:
            if current_length + len(sentence) > self.chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_length = len(sentence)
            else:
                current_chunk.append(sentence)
                current_length += len(sentence)
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks

    def _process_chunk_with_retries(self, chunk: str, context: Dict) -> Optional[Dict]:
        max_retries = 5  # ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’å¢—ã‚„ã™
        base_wait_time = 3  # åŸºæœ¬å¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™
        
        for attempt in range(max_retries):
            try:
                self.rate_limiter.wait()
                
                response = self.model.generate_content(
                    self._create_summary_prompt(chunk, context),
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.2,
                        top_p=0.95,
                        top_k=40,
                        max_output_tokens=8192,
                    )
                )
                
                if not response.text:
                    raise ValueError("Empty response received")
                
                result = self._validate_json_response(response.text)
                if result:
                    self.rate_limiter.report_success()
                    return result
                
                raise ValueError("Invalid JSON response")
                
            except Exception as e:
                error_msg = str(e).lower()
                self.rate_limiter.report_error()
                
                if "quota" in error_msg or "429" in error_msg:
                    self.rate_limiter.report_quota_exceeded()
                    wait_time = base_wait_time * (2 ** attempt)
                    logger.warning(f"API quota exceeded, waiting {wait_time} seconds...")
                    time.sleep(wait_time)
                    
                    # APIã‚­ãƒ¼ã®å†åˆæœŸåŒ–ã‚’è©¦ã¿ã‚‹
                    try:
                        self._initialize_api()
                    except Exception as key_error:
                        logger.error(f"Failed to reinitialize API: {str(key_error)}")
                
                if attempt < max_retries - 1:
                    wait_time = base_wait_time * (2 ** attempt)
                    logger.warning(f"Retrying chunk processing ({max_retries - attempt - 1} attempts left)")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Failed to process chunk after all retries: {str(e)}")
                    return None
        
        return None

    def _extract_video_id(self, url: str) -> str:
        """Extract video ID from YouTube URL"""
        video_id_match = re.search(r'(?:v=|/v/|youtu\.be/)([^&?/]+)', url)
        if not video_id_match:
            raise ValueError("Invalid YouTube URL format")
        return video_id_match.group(1)

    def get_transcript(self, url: str) -> str:
        """Get transcript from YouTube video with improved error handling"""
        try:
            video_id = self._extract_video_id(url)
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # Try to get Japanese transcript first
            try:
                transcript = transcript_list.find_transcript(['ja'])
            except Exception:
                # If Japanese is not available, try English
                try:
                    transcript = transcript_list.find_transcript(['en'])
                except Exception:
                    # If no preferred language is available, get the first available
                    transcript = transcript_list.find_manually_created_transcript()
            
            transcript_data = transcript.fetch()
            formatter = TextFormatter()
            formatted_transcript = formatter.format_transcript(transcript_data)
            
            if not formatted_transcript:
                raise ValueError("ç©ºã®æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ãŒè¿”ã•ã‚Œã¾ã—ãŸ")
            
            return formatted_transcript
            
        except Exception as e:
            logger.error(f"Error getting transcript: {str(e)}")
            raise Exception(f"æ–‡å­—èµ·ã“ã—ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    def generate_summary(self, text: str, progress_callback: Optional[Callable] = None) -> str:
        """Generate summary with improved context handling and progress tracking"""
        try:
            if not text:
                raise ValueError("Input text is empty")

            # Generate cache key based on text content
            cache_key = hashlib.md5(text.encode()).hexdigest()
            if cache_key in self.summary_cache:
                return self.summary_cache[cache_key]

            # Split text into chunks with context preservation
            chunks = self._split_text_into_chunks(text)
            total_chunks = len(chunks)
            summaries = []

            for i, chunk in enumerate(chunks, 1):
                if progress_callback:
                    progress = (i - 1) / total_chunks
                    progress_callback(progress, f"ãƒãƒ£ãƒ³ã‚¯ {i}/{total_chunks} ã‚’å‡¦ç†ä¸­...")

                # Get context from previous summaries
                context = self._get_chunk_context(summaries)
                
                # Process chunk with retries and context
                chunk_summary = self._process_chunk_with_retries(chunk, context)
                if chunk_summary:
                    summaries.append(chunk_summary)
                    self._update_context_memory(chunk_summary)

            if not summaries:
                raise ValueError("No valid summaries generated")

            # Combine summaries with context awareness
            combined_summary = self._combine_chunk_summaries(summaries)
            
            if progress_callback:
                progress_callback(1.0, "âœ¨ è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸ")

            # Format the final summary
            final_summary = self._format_final_summary(combined_summary)
            self.summary_cache[cache_key] = final_summary
            return final_summary

        except Exception as e:
            logger.error(f"Error in summary generation: {str(e)}")
            if progress_callback:
                progress_callback(1.0, f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def _get_chunk_context(self, previous_summaries: List[Dict]) -> Dict:
        """Get context information from previous chunk summaries"""
        if not previous_summaries:
            return {}

        # Get the most recent summary
        latest_summary = previous_summaries[-1]
        
        # Extract relevant context
        context = {
            "previous_topics": [],
            "key_points": [],
            "continuing_themes": []
        }

        if "æ–‡è„ˆé€£æº" in latest_summary:
            context["continuing_themes"] = latest_summary["æ–‡è„ˆé€£æº"].get("ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯", [])
            context["new_topics"] = latest_summary["æ–‡è„ˆé€£æº"].get("æ–°è¦ãƒˆãƒ”ãƒƒã‚¯", [])

        if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in latest_summary:
            context["key_points"] = [
                point["ã‚¿ã‚¤ãƒˆãƒ«"] for point in latest_summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]
            ]

        return context

    def _format_final_summary(self, combined_summary: Dict) -> str:
        """Format the combined summary into a readable markdown string"""
        sections = []

        # Add overview
        if "æ¦‚è¦" in combined_summary:
            sections.append(f"## æ¦‚è¦\n\n{combined_summary['æ¦‚è¦'].strip()}\n")

        # Add main points
        if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in combined_summary and combined_summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
            sections.append("## ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ\n")
            for point in combined_summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                if isinstance(point, dict):
                    title = point.get("ã‚¿ã‚¤ãƒˆãƒ«", "")
                    desc = point.get("èª¬æ˜", "")
                    importance = point.get("é‡è¦åº¦", 3)
                    sections.append(f"### {title} {'ğŸ”¥' * importance}\n{desc}\n")

        # Add detailed analysis
        if "è©³ç´°åˆ†æ" in combined_summary and combined_summary["è©³ç´°åˆ†æ"]:
            sections.append("## è©³ç´°åˆ†æ\n")
            for analysis in combined_summary["è©³ç´°åˆ†æ"]:
                if isinstance(analysis, dict):
                    section = analysis.get("ã‚»ã‚¯ã‚·ãƒ§ãƒ³", "")
                    content = analysis.get("å†…å®¹", "")
                    points = analysis.get("ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ", [])
                    sections.append(f"### {section}\n{content}\n")
                    if points:
                        sections.append("\n**ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ:**\n")
                        for point in points:
                            sections.append(f"- {point}\n")

        # Add keywords
        if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in combined_summary and combined_summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
            sections.append("## ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\n")
            for keyword in combined_summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                if isinstance(keyword, dict):
                    term = keyword.get("ç”¨èª", "")
                    desc = keyword.get("èª¬æ˜", "")
                    importance = keyword.get("æ–‡è„ˆä¸Šã®é‡è¦åº¦", 3)
                    sections.append(f"### {term} {'â­' * importance}\n{desc}\n")

        return "\n".join(sections)

    def proofread_text(self, text: str, progress_callback: Optional[Callable] = None) -> str:
        """Proofread and enhance text with context awareness"""
        try:
            if not text:
                raise ValueError("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")

            if progress_callback:
                progress_callback(0.2, "ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æä¸­...")

            # Create context-aware prompt for proofreading
            context = self._get_context_summary()
            prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¡æ­£ã—ã€èª­ã¿ã‚„ã™ãæ•´å½¢ã—ã¦ãã ã•ã„ã€‚æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦æ”¹å–„ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:
{context}

å¿…è¦ãªæ”¹å–„ç‚¹:
1. æ–‡ç« ã®æ§‹é€ ã¨æµã‚Œã®æ”¹å–„
2. æ–‡ä½“ã®çµ±ä¸€
3. å†—é•·ãªè¡¨ç¾ã®å‰Šé™¤
4. é©åˆ‡ãªæ®µè½åˆ†ã‘
5. æ–‡è„ˆã®ä¸€è²«æ€§ç¢ºä¿

çµæœã¯æ•´å½¢æ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"""

            if progress_callback:
                progress_callback(0.5, "ãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¡æ­£ä¸­...")

            # Process with retries
            response = None
            for attempt in range(self.max_retries):
                try:
                    self.rate_limiter.wait()
                    response = self.model.generate_content(
                        prompt,
                        generation_config=genai.types.GenerationConfig(
                            temperature=0.3,
                            top_p=0.8,
                            top_k=40,
                            max_output_tokens=8192,
                        )
                    )
                    break
                except Exception as e:
                    if attempt == self.max_retries - 1:
                        raise
                    time.sleep(self.backoff_factor ** attempt)

            if not response or not response.text:
                raise ValueError("æ ¡æ­£çµæœã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

            if progress_callback:
                progress_callback(1.0, "âœ¨ ãƒ†ã‚­ã‚¹ãƒˆã®æ ¡æ­£ãŒå®Œäº†ã—ã¾ã—ãŸ")

            return response.text.strip()

        except Exception as e:
            logger.error(f"Error in text proofreading: {str(e)}")
            if progress_callback:
                progress_callback(1.0, f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def _update_context_memory(self, summary: Dict[str, Any]) -> None:
        """Update context memory with new summary information"""
        self.context_memory.append({
            "æ¦‚è¦": summary.get("æ¦‚è¦", ""),
            "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [point["ã‚¿ã‚¤ãƒˆãƒ«"] for point in summary.get("ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ", [])],
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [kw["ç”¨èª"] for kw in summary.get("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", [])]
        })
        
        # Keep only the most recent context
        if len(self.context_memory) > self.max_context_memory:
            self.context_memory.pop(0)

    def _get_context_summary(self) -> str:
        """Generate a summary of the current context"""
        if not self.context_memory:
            return ""
            
        context_summary = {
            "ã“ã‚Œã¾ã§ã®æ¦‚è¦": [ctx["æ¦‚è¦"] for ctx in self.context_memory],
            "é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ": list(set(
                point for ctx in self.context_memory 
                for point in ctx["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]
            )),
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": list(set(
                kw for ctx in self.context_memory 
                for kw in ctx["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]
            ))
        }
        
        return json.dumps(context_summary, ensure_ascii=False, indent=2)

    def _validate_json_response(self, response_text: str) -> Optional[Dict]:
        """Improved JSON response validation with multiple recovery attempts"""
        if not response_text:
            logger.warning("Empty response received")
            return None

        try:
            # Remove any leading/trailing non-JSON content
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            if start_idx >= 0 and end_idx > 0:
                json_str = response_text[start_idx:end_idx]
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError as e:
                    logger.warning(f"Initial JSON parsing failed: {str(e)}")
                    
                    # Try to fix common JSON issues
                    json_str = json_str.replace('\n', ' ').replace('\r', '')
                    json_str = re.sub(r'(?<!\\)"(?!,|\s*}|\s*])', '\\"', json_str)
                    try:
                        return json.loads(json_str)
                    except json.JSONDecodeError:
                        logger.warning("Failed to fix JSON structure")
                        return None
            else:
                logger.warning("No JSON structure found in response")
                return None
        except Exception as e:
            logger.warning(f"Unexpected error in JSON validation: {str(e)}")
            return None

    def _create_summary_prompt(self, text: str, context: Optional[Dict] = None) -> str:
        """Enhanced prompt with context awareness and improved structure"""
        context_info = ""
        if context:
            context_info = f"\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:\n{json.dumps(context, ensure_ascii=False, indent=2)}"

        previous_context = self._get_context_summary()
        if previous_context:
            context_info += f"\n\nå‰å›ã¾ã§ã®æ–‡è„ˆ:\n{previous_context}"

        prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€å‰å¾Œã®æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸä¸Šã§ã€å³å¯†ãªJSONå½¢å¼ã§æ§‹é€ åŒ–ã•ã‚ŒãŸè¦ç´„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ:
{text}
{context_info}

å¿…é ˆå‡ºåŠ›å½¢å¼:
{{
    "æ¦‚è¦": "150æ–‡å­—ä»¥å†…ã®ç°¡æ½”ãªèª¬æ˜ï¼ˆå‰å¾Œã®æ–‡è„ˆã‚’è€ƒæ…®ï¼‰",
    "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [
        {{
            "ã‚¿ã‚¤ãƒˆãƒ«": "é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã®è¦‹å‡ºã—",
            "èª¬æ˜": "å…·ä½“çš„ãªèª¬æ˜æ–‡ï¼ˆå‰å¾Œã®æ–‡è„ˆã¨ã®é–¢é€£æ€§ã‚’å«ã‚€ï¼‰",
            "é‡è¦åº¦": "1-5ã®æ•°å€¤"
        }}
    ],
    "è©³ç´°åˆ†æ": [
        {{
            "ã‚»ã‚¯ã‚·ãƒ§ãƒ³": "åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®åç§°",
            "å†…å®¹": "è©³ç´°ãªåˆ†æå†…å®¹ï¼ˆæ–‡è„ˆã‚’è€ƒæ…®ï¼‰",
            "ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ": [
                "é‡è¦ãªç‚¹1",
                "é‡è¦ãªç‚¹2"
            ],
            "å‰å¾Œã®é–¢é€£æ€§": "å‰å¾Œã®æ–‡è„ˆã¨ã®é–¢ä¿‚æ€§ã®èª¬æ˜"
        }}
    ],
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [
        {{
            "ç”¨èª": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            "èª¬æ˜": "ç°¡æ½”ãªèª¬æ˜",
            "é–¢é€£èª": ["é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2"],
            "æ–‡è„ˆä¸Šã®é‡è¦åº¦": "1-5ã®æ•°å€¤"
        }}
    ],
    "æ–‡è„ˆé€£æº": {{
        "å‰ã®å†…å®¹ã¨ã®é–¢é€£": "å‰ã®éƒ¨åˆ†ã¨ã®é–¢é€£æ€§ã®èª¬æ˜",
        "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯": ["ç¶™ç¶šã—ã¦é‡è¦ãªè©±é¡Œ1", "ç¶™ç¶šã—ã¦é‡è¦ãªè©±é¡Œ2"],
        "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯": ["æ–°ã—ãå°å…¥ã•ã‚ŒãŸè©±é¡Œ1", "æ–°ã—ãå°å…¥ã•ã‚ŒãŸè©±é¡Œ2"]
    }}
}}

åˆ¶ç´„äº‹é …:
1. å¿…ãšæœ‰åŠ¹ãªJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¶­æŒã™ã‚‹ã“ã¨
2. ã™ã¹ã¦ã®æ–‡å­—åˆ—ã¯é©åˆ‡ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã™ã‚‹ã“ã¨
3. æ•°å€¤ã¯å¿…ãšæ•°å€¤å‹ã§å‡ºåŠ›ã™ã‚‹ã“ã¨
4. é…åˆ—ã¯å¿…ãš1ã¤ä»¥ä¸Šã®è¦ç´ ã‚’å«ã‚€ã“ã¨
5. ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã¯3-5é …ç›®ã‚’å«ã‚€ã“ã¨
6. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯æœ€ä½3ã¤å«ã‚€ã“ã¨
7. å‰å¾Œã®æ–‡è„ˆã‚’å¿…ãšè€ƒæ…®ã™ã‚‹ã“ã¨
8. æ–‡è„ˆé€£æºã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…é ˆ

æ³¨æ„:
- JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä»¥å¤–ã®è£…é£¾ã‚„èª¬æ˜ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å¿…é ˆã§ã™ã€‚çœç•¥ã—ãªã„ã§ãã ã•ã„
- ä¸æ­£ãªJSONæ§‹é€ ã‚’é¿ã‘ã‚‹ãŸã‚ã€æ–‡å­—åˆ—å†…ã®äºŒé‡å¼•ç”¨ç¬¦ã¯å¿…ãšã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¦ãã ã•ã„
- å‰å¾Œã®æ–‡è„ˆã¨ã®ä¸€è²«æ€§ã‚’é‡è¦–ã—ã¦ãã ã•ã„"""

        return prompt

    def _process_chunk_with_retries(self, chunk: str, context: Dict) -> Optional[Dict]:
        """Process text chunk with improved error handling and context awareness"""
        remaining_retries = self.max_retries
        backoff_time = 1

        while remaining_retries > 0:
            try:
                self.rate_limiter.wait()
                
                response = self.model.generate_content(
                    self._create_summary_prompt(chunk, context),
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.2,
                        top_p=0.95,
                        top_k=40,
                        max_output_tokens=8192,
                    )
                )
                
                if not response.text:
                    raise ValueError("Empty response received")
                
                result = self._validate_json_response(response.text)
                if result:
                    self.rate_limiter.report_success()
                    self._update_context_memory(result)
                    return result
                
                raise ValueError("Invalid JSON response")
                
            except Exception as e:
                error_msg = str(e).lower()
                remaining_retries -= 1
                
                if "quota" in error_msg or "429" in error_msg:
                    self.rate_limiter.report_quota_exceeded()
                    logger.warning(f"API quota exceeded, waiting {backoff_time} seconds...")
                    time.sleep(backoff_time)
                    backoff_time *= 2
                    
                    try:
                        self._initialize_api()
                    except Exception as key_error:
                        logger.error(f"Failed to reinitialize API: {str(key_error)}")
                else:
                    self.rate_limiter.report_error()
                
                if remaining_retries > 0:
                    logger.warning(f"Retrying chunk processing ({remaining_retries} attempts left)")
                else:
                    logger.error(f"Failed to process chunk after all retries: {str(e)}")
                    return None
        
        return None

    def _split_text_into_chunks(self, text: str) -> List[str]:
        """Split text into chunks with improved context preservation"""
        if not text:
            raise ValueError("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")
            
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        text = text.replace('\n', ' ').strip()
        text = re.sub(r'\s+', ' ', text)
        
        if len(text) < 200:
            return [text]
        
        # æ–‡å˜ä½ã§ã®åˆ†å‰²ï¼ˆå¥èª­ç‚¹ã‚‚è€ƒæ…®ï¼‰
        sentences = []
        temp = ''
        for char in text:
            temp += char
            if char in ['ã€‚', 'ï¼', 'ï¼Ÿ', '!', '?', '.']:
                if temp.strip():
                    sentences.append(temp.strip())
                temp = ''
        if temp.strip():
            sentences.append(temp.strip())
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue
                
            sentence_length = len(sentence)
            
            # 1æ–‡ãŒé•·ã™ãã‚‹å ´åˆã¯é©åˆ‡ãªä½ç½®ã§åˆ†å‰²
            if sentence_length > self.chunk_size:
                sub_chunks = [sentence[i:i+self.chunk_size] 
                             for i in range(0, len(sentence), self.chunk_size)]
                for sub_chunk in sub_chunks:
                    if sub_chunk:
                        chunks.append(sub_chunk)
                continue
            
            # ãƒãƒ£ãƒ³ã‚¯ã®é‡è¤‡ã‚’è€ƒæ…®ã—ãŸå‡¦ç†
            if current_length + sentence_length > self.chunk_size - self.overlap_size and current_chunk:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                chunks.append(' '.join(current_chunk))
                
                # é‡è¤‡éƒ¨åˆ†ã‚’æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã®é–‹å§‹ç‚¹ã¨ã—ã¦ä½¿ç”¨
                overlap_sentences = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
                current_chunk = overlap_sentences.copy()
                current_length = sum(len(s) for s in current_chunk)
            
            current_chunk.append(sentence)
            current_length += sentence_length
        
        # æ®‹ã‚Šã®æ–‡ã‚’å‡¦ç†
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯
        if not chunks:
            return [text]
        
        return chunks

    def _combine_chunk_summaries(self, summaries: List[Dict]) -> Dict:
        """Improved chunk summary combination with context awareness"""
        if not summaries:
            raise ValueError("No valid summaries to combine")

        combined = {
            "æ¦‚è¦": "",
            "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [],
            "è©³ç´°åˆ†æ": [],
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [],
            "æ–‡è„ˆé€£æº": {
                "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯": set(),
                "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯": set()
            }
        }

        # æ–‡è„ˆã®é€£ç¶šæ€§ã‚’è¿½è·¡
        continuous_topics = set()
        new_topics = set()

        for summary in summaries:
            if not isinstance(summary, dict):
                continue

            # æ¦‚è¦ã®çµåˆ
            if "æ¦‚è¦" in summary:
                combined["æ¦‚è¦"] += summary["æ¦‚è¦"].strip() + " "

            # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã®çµ±åˆï¼ˆé‡è¤‡ã‚’è€ƒæ…®ï¼‰
            if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in summary:
                for point in summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                    if isinstance(point, dict) and "ã‚¿ã‚¤ãƒˆãƒ«" in point and "èª¬æ˜" in point:
                        # æ—¢å­˜ã®ãƒã‚¤ãƒ³ãƒˆã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if not any(p["ã‚¿ã‚¤ãƒˆãƒ«"] == point["ã‚¿ã‚¤ãƒˆãƒ«"] for p in combined["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]):
                            combined["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"].append(point)

            # è©³ç´°åˆ†æã®çµ±åˆ
            if "è©³ç´°åˆ†æ" in summary:
                for analysis in summary["è©³ç´°åˆ†æ"]:
                    if isinstance(analysis, dict) and "ã‚»ã‚¯ã‚·ãƒ§ãƒ³" in analysis:
                        combined["è©³ç´°åˆ†æ"].append({
                            "ã‚»ã‚¯ã‚·ãƒ§ãƒ³": analysis["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"],
                            "å†…å®¹": analysis["å†…å®¹"],
                            "ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ": analysis.get("ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ", []),
                            "å‰å¾Œã®é–¢é€£æ€§": analysis.get("å‰å¾Œã®é–¢é€£æ€§", "")
                        })

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®çµ±åˆï¼ˆé‡è¤‡ã‚’é™¤å»ï¼‰
            if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in summary:
                for keyword in summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                    if isinstance(keyword, dict) and "ç”¨èª" in keyword:
                        if not any(k["ç”¨èª"] == keyword["ç”¨èª"] for k in combined["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]):
                            combined["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].append(keyword)

            # æ–‡è„ˆé€£æºæƒ…å ±ã®çµ±åˆ
            if "æ–‡è„ˆé€£æº" in summary:
                context_info = summary["æ–‡è„ˆé€£æº"]
                if isinstance(context_info, dict):
                    if "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯" in context_info:
                        continuous_topics.update(context_info["ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"])
                    if "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯" in context_info:
                        new_topics.update(context_info["æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"])

        # æœ€çµ‚çš„ãªæ–‡è„ˆé€£æºæƒ…å ±ã®è¨­å®š
        combined["æ–‡è„ˆé€£æº"] = {
            "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯": list(continuous_topics),
            "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯": list(new_topics)
        }

        # æ¦‚è¦ã®æ•´å½¢
        combined["æ¦‚è¦"] = combined["æ¦‚è¦"].strip()[:150]

        # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã®åˆ¶é™
        combined["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"] = sorted(
            combined["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"],
            key=lambda x: x.get("é‡è¦åº¦", 0),
            reverse=True
        )[:5]

        return combined

    def generate_summary(self, text: str, progress_callback: Optional[Callable] = None) -> str:
        """Generate context-aware summary with improved processing"""
        try:
            if not text:
                raise ValueError("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")

            if progress_callback:
                progress_callback(0.1, "ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æä¸­...")

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache_key = hashlib.md5(text.encode()).hexdigest()
            if cache_key in self.cache:
                if progress_callback:
                    progress_callback(1.0, "âœ¨ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è¦ç´„ã‚’å–å¾—ã—ã¾ã—ãŸ")
                return self.cache[cache_key]

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ¢ãƒªã®åˆæœŸåŒ–
            self.context_memory = []

            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self._split_text_into_chunks(text)
            chunk_summaries = []
            failed_chunks = 0

            for i, chunk in enumerate(chunks, 1):
                if progress_callback:
                    progress = 0.1 + (0.7 * (i / len(chunks)))
                    progress_callback(progress, f"ãƒãƒ£ãƒ³ã‚¯ {i}/{len(chunks)} ã‚’å‡¦ç†ä¸­...")

                context = {
                    "total_chunks": len(chunks),
                    "current_chunk": i,
                    "chunk_position": "é–‹å§‹" if i == 1 else "çµ‚äº†" if i == len(chunks) else "ä¸­é–“",
                    "previous_summaries": self._get_context_summary() if self.context_memory else ""
                }

                chunk_summary = self._process_chunk_with_retries(chunk, context)
                
                if chunk_summary:
                    chunk_summaries.append(chunk_summary)
                else:
                    failed_chunks += 1
                    logger.warning(f"Failed to process chunk {i} after all retries")

            if not chunk_summaries:
                raise Exception("ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")

            if failed_chunks > 0:
                logger.warning(f"{failed_chunks} chunks failed to process")

            if progress_callback:
                progress_callback(0.9, "è¦ç´„ã‚’çµ±åˆä¸­...")

            # æœ€çµ‚çš„ãªè¦ç´„ã‚’ç”Ÿæˆ
            try:
                final_summary_data = self._combine_chunk_summaries(chunk_summaries)
            except Exception as e:
                raise Exception(f"è¦ç´„ã®çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

            # æœ€çµ‚çš„ãªè¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã®ç”Ÿæˆ
            final_summary = self._format_final_summary(final_summary_data)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.cache[cache_key] = final_summary

            if progress_callback:
                progress_callback(1.0, "âœ¨ è¦ç´„ãŒå®Œäº†ã—ã¾ã—ãŸ")

            return final_summary

        except Exception as e:
            logger.error(f"Error in summary generation: {str(e)}")
            if progress_callback:
                progress_callback(1.0, f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise

    def _format_final_summary(self, summary_data: Dict) -> str:
        """Format the final summary with improved structure and context awareness"""
        sections = []
        
        # æ¦‚è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        sections.append(f"# æ¦‚è¦\n{summary_data['æ¦‚è¦']}\n")
        
        # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ
        sections.append("# ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ")
        for point in summary_data["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
            sections.append(f"## {point['ã‚¿ã‚¤ãƒˆãƒ«']}")
            sections.append(f"{point['èª¬æ˜']}")
            sections.append(f"é‡è¦åº¦: {'â­' * int(point['é‡è¦åº¦'])}\n")
        
        # è©³ç´°åˆ†æ
        sections.append("# è©³ç´°åˆ†æ")
        for analysis in summary_data["è©³ç´°åˆ†æ"]:
            sections.append(f"## {analysis['ã‚»ã‚¯ã‚·ãƒ§ãƒ³']}")
            sections.append(analysis['å†…å®¹'])
            if analysis['ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ']:
                sections.append("\nä¸»ãªãƒã‚¤ãƒ³ãƒˆ:")
                for point in analysis['ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ']:
                    sections.append(f"- {point}")
            if "å‰å¾Œã®é–¢é€£æ€§" in analysis:
                sections.append(f"\nå‰å¾Œã®é–¢é€£æ€§: {analysis['å‰å¾Œã®é–¢é€£æ€§']}\n")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        sections.append("# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
        for keyword in summary_data["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
            sections.append(f"## {keyword['ç”¨èª']}")
            sections.append(f"{keyword['èª¬æ˜']}")
            if keyword.get('é–¢é€£èª'):
                sections.append("é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: " + ", ".join(keyword['é–¢é€£èª']))
            if "æ–‡è„ˆä¸Šã®é‡è¦åº¦" in keyword:
                sections.append(f"æ–‡è„ˆä¸Šã®é‡è¦åº¦: {'â­' * int(keyword['æ–‡è„ˆä¸Šã®é‡è¦åº¦'])}\n")
        
        # æ–‡è„ˆé€£æº
        if "æ–‡è„ˆé€£æº" in summary_data:
            sections.append("# æ–‡è„ˆã®é€£ç¶šæ€§")
            context_info = summary_data["æ–‡è„ˆé€£æº"]
            
            if context_info.get("ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"):
                sections.append("\n## ç¶™ç¶šã—ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯")
                for topic in context_info["ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"]:
                    sections.append(f"- {topic}")
            
            if context_info.get("æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"):
                sections.append("\n## æ–°ã—ãå°å…¥ã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯")
                for topic in context_info["æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"]:
                    sections.append(f"- {topic}")
        
        return "\n".join(sections)
import os
import google.generativeai as genai
import logging
import json
from typing import List, Dict, Optional
from youtube_transcript_api import YouTubeTranscriptApi
from cachetools import TTLCache

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TextProcessor:
    def __init__(self):
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("Gemini API key is not set in environment variables")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-pro')
        self._cache = TTLCache(maxsize=100, ttl=3600)  # 1-hour cache

    def _build_topic_hierarchy(self, summaries: List[Dict]) -> Dict:
        """Build topic hierarchy from previous summaries"""
        hierarchy = {
            "main_topics": [],
            "subtopics": {},
            "relationships": []
        }
        
        for summary in summaries:
            if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in summary:
                for point in summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                    topic = point.get("ã‚¿ã‚¤ãƒˆãƒ«", "")
                    if topic and topic not in hierarchy["main_topics"]:
                        hierarchy["main_topics"].append(topic)
                        hierarchy["subtopics"][topic] = []
                    
            if "è©³ç´°åˆ†æ" in summary:
                for analysis in summary["è©³ç´°åˆ†æ"]:
                    section = analysis.get("ã‚»ã‚¯ã‚·ãƒ§ãƒ³", "")
                    points = analysis.get("ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ", [])
                    if section in hierarchy["main_topics"]:
                        hierarchy["subtopics"][section].extend(points)
                    
        return hierarchy

    def _get_chunk_context(self, previous_summaries: List[Dict]) -> Dict:
        """Get context from previous chunks for better continuity"""
        context = {
            "previous_topics": [],
            "key_points": [],
            "continuing_themes": [],
            "topic_hierarchy": {},
            "importance_scores": {},
            "related_terms": {}
        }
        
        if not previous_summaries:
            return context

        # Build topic hierarchy from previous summaries
        context["topic_hierarchy"] = self._build_topic_hierarchy(previous_summaries)
        
        # Analyze last 3 summaries for immediate context
        recent_summaries = previous_summaries[-3:]
        
        for summary in recent_summaries:
            # Track topics and their importance
            if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in summary:
                for point in summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                    topic = point.get("ã‚¿ã‚¤ãƒˆãƒ«", "")
                    importance = point.get("é‡è¦åº¦", 1)
                    if topic:
                        context["previous_topics"].append(topic)
                        context["importance_scores"][topic] = importance
            
            # Track continuing themes
            if "æ–‡è„ˆé€£æº" in summary:
                context["continuing_themes"].extend(
                    summary["æ–‡è„ˆé€£æº"].get("ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯", [])
                )
                
            # Build related terms dictionary
            if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in summary:
                for keyword in summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                    term = keyword.get("ç”¨èª", "")
                    if term:
                        context["related_terms"][term] = keyword.get("èª¬æ˜", "")
        
        # Remove duplicates while preserving order
        context["previous_topics"] = list(dict.fromkeys(context["previous_topics"]))
        context["continuing_themes"] = list(dict.fromkeys(context["continuing_themes"]))
        
        return context

    def _chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """Split text into manageable chunks while preserving context"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        
        for word in words:
            word_length = len(word)
            if current_length + word_length > chunk_size and current_chunk:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(word)
            current_length += word_length + 1  # +1 for space
            
        if current_chunk:
            chunks.append(" ".join(current_chunk))
            
        return chunks

    def get_transcript(self, youtube_url: str) -> str:
        """Get transcript from YouTube video"""
        try:
            # Extract video ID from URL
            video_id = youtube_url.split("v=")[1].split("&")[0]
            
            # Check cache first
            cache_key = f"transcript_{video_id}"
            if cache_key in self._cache:
                return self._cache[cache_key]
            
            # Get transcript
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ja', 'en'])
            full_text = " ".join([entry['text'] for entry in transcript])
            
            # Cache the result
            self._cache[cache_key] = full_text
            return full_text
            
        except Exception as e:
            logger.error(f"Error getting transcript: {str(e)}")
            raise Exception(f"Failed to get transcript: {str(e)}")

    def generate_summary(self, text: str) -> str:
        """Generate context-aware summary using Gemini API"""
        try:
            chunks = self._chunk_text(text)
            summaries = []
            previous_summaries = []
            
            for i, chunk in enumerate(chunks):
                # Get context from previous summaries
                context = self._get_chunk_context(previous_summaries)
                
                # Build prompt with context
                prompt = f'''
                ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã€JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
                å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®æ–‡è„ˆæƒ…å ±ï¼š
                - ç¶™ç¶šä¸­ã®ãƒˆãƒ”ãƒƒã‚¯: {", ".join(context["continuing_themes"])}
                - é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {json.dumps(context["related_terms"], ensure_ascii=False)}
                
                ãƒ†ã‚­ã‚¹ãƒˆ:
                {chunk}
                
                å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
                {{
                    "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [
                        {{"ã‚¿ã‚¤ãƒˆãƒ«": "ãƒˆãƒ”ãƒƒã‚¯", "é‡è¦åº¦": 1-5ã®æ•°å€¤}}
                    ],
                    "è©³ç´°åˆ†æ": [
                        {{"ã‚»ã‚¯ã‚·ãƒ§ãƒ³": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å", "ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ": ["ãƒã‚¤ãƒ³ãƒˆ1", "ãƒã‚¤ãƒ³ãƒˆ2"]}}
                    ],
                    "æ–‡è„ˆé€£æº": {{
                        "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯": ["ãƒˆãƒ”ãƒƒã‚¯1", "ãƒˆãƒ”ãƒƒã‚¯2"],
                        "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯": ["æ–°ãƒˆãƒ”ãƒƒã‚¯1", "æ–°ãƒˆãƒ”ãƒƒã‚¯2"]
                    }},
                    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [
                        {{"ç”¨èª": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "èª¬æ˜": "èª¬æ˜æ–‡"}}
                    ]
                }}
                '''
                
                response = self.model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.3,
                        top_p=0.8,
                        top_k=40,
                        max_output_tokens=8192,
                    )
                )
                
                try:
                    summary_data = json.loads(response.text)
                    previous_summaries.append(summary_data)
                    
                    # Format summary for display
                    formatted_summary = self._format_summary(summary_data, i == 0)
                    summaries.append(formatted_summary)
                except json.JSONDecodeError:
                    logger.warning(f"Failed to parse JSON from chunk {i}, using raw text")
                    summaries.append(response.text)
            
            return "\n\n".join(summaries)
            
        except Exception as e:
            logger.error(f"Error generating summary: {str(e)}")
            raise Exception(f"Failed to generate summary: {str(e)}")

    def _format_summary(self, summary_data: Dict, is_first_chunk: bool = False) -> str:
        """Format summary data into readable markdown"""
        lines = []
        
        # Add main points
        if is_first_chunk:
            lines.append("# ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¦ç´„")
            
        if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in summary_data:
            lines.append("\n## ğŸ¯ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ")
            for point in summary_data["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                stars = "â­" * point.get("é‡è¦åº¦", 1)
                lines.append(f"- **{point['ã‚¿ã‚¤ãƒˆãƒ«']}** {stars}")
        
        # Add detailed analysis
        if "è©³ç´°åˆ†æ" in summary_data:
            lines.append("\n## ğŸ“Š è©³ç´°åˆ†æ")
            for analysis in summary_data["è©³ç´°åˆ†æ"]:
                lines.append(f"\n### {analysis['ã‚»ã‚¯ã‚·ãƒ§ãƒ³']}")
                for point in analysis['ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ']:
                    lines.append(f"- {point}")
        
        # Add keywords
        if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in summary_data:
            lines.append("\n## ğŸ” é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            for keyword in summary_data["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                lines.append(f"- **{keyword['ç”¨èª']}**: {keyword['èª¬æ˜']}")
        
        return "\n".join(lines)

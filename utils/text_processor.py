import os
import json
import google.generativeai as genai
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional
import logging
import re
import time

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TextProcessor:
    def __init__(self, chunk_size=1500, overlap_size=200):
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("Gemini API key is not set in environment variables")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        self._cache = {}
        self.chunk_size = chunk_size
        self.overlap_size = overlap_size
        
    def get_transcript(self, url: str) -> str:
        try:
            video_id = self._extract_video_id(url)
            
            # Check cache first
            cache_key = f"transcript_{video_id}"
            if cache_key in self._cache:
                return self._cache[cache_key]
            
            # Get transcript with better formatting
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            transcript = transcript_list.find_transcript(['ja', 'en'])
            transcript_data = transcript.fetch()
            formatter = TextFormatter()
            formatted_transcript = formatter.format_transcript(transcript_data)
            
            # Cache and return
            self._cache[cache_key] = formatted_transcript
            return formatted_transcript
            
        except Exception as e:
            logger.error(f"å­—å¹•ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise Exception(f"å­—å¹•ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            
    def _extract_video_id(self, url: str) -> str:
        match = re.search(r'(?:v=|/v/|youtu\.be/)([^&?/]+)', url)
        if not match:
            raise ValueError("ç„¡åŠ¹ãªYouTube URLå½¢å¼ã§ã™")
        return match.group(1)

    def generate_summary(self, text: str) -> str:
        try:
            chunks = self._split_text_into_chunks(text)
            
            # Process chunks in parallel
            with ThreadPoolExecutor() as executor:
                summaries = list(executor.map(self._process_chunk, chunks))
            
            # Filter out None values and combine summaries
            valid_summaries = [s for s in summaries if s is not None]
            
            if not valid_summaries:
                raise ValueError("æœ‰åŠ¹ãªè¦ç´„ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
            return self._format_summaries(valid_summaries)
            
        except Exception as e:
            logger.error(f"è¦ç´„ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise Exception(f"è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    def _split_text_into_chunks(self, text: str) -> List[str]:
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s*', text)
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            if current_length + len(sentence) > self.chunk_size:
                chunks.append("".join(current_chunk))
                # Keep overlap for context
                current_chunk = current_chunk[-self.overlap_size:]
                current_length = sum(len(s) for s in current_chunk)
            current_chunk.append(sentence)
            current_length += len(sentence)
            
        if current_chunk:
            chunks.append("".join(current_chunk))
            
        return chunks

    def _process_chunk(self, chunk: str) -> Optional[Dict]:
        max_retries = 3
        base_wait_time = 5
        
        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(
                    self._create_summary_prompt(chunk, {}),
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.3,
                        top_p=0.8,
                        top_k=40,
                        max_output_tokens=8192,
                    )
                )
                
                if response.text:
                    result = self._validate_json_response(response.text)
                    if result:
                        return result
                        
            except Exception as e:
                logger.warning(f"Chunk processing attempt {attempt + 1} failed: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(base_wait_time * (2 ** attempt))
                    
        return None

    def _validate_json_response(self, response_text: str) -> dict:
        """JSON responseã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            # Clean up the response text
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
                
            # Try to parse JSON
            data = json.loads(cleaned_text)
            
            # Validate required fields
            required_fields = ["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ", "è©³ç´°åˆ†æ", "æ–‡è„ˆé€£æº", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]
            if not all(field in data for field in required_fields):
                logger.warning("Missing required fields in JSON response")
                return None
                
            return data
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON response: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"Error validating JSON response: {str(e)}")
            return None

    def _get_chunk_context(self, previous_summaries: list) -> dict:
        """å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡º"""
        context = {
            "continuing_themes": [],
            "key_themes": [],
            "importance_factors": []
        }
        
        if not previous_summaries:
            return context
            
        # Get themes from previous summaries
        for summary in previous_summaries[-2:]:  # Look at last 2 summaries
            if "æ–‡è„ˆé€£æº" in summary:
                context["continuing_themes"].extend(summary["æ–‡è„ˆé€£æº"].get("ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯", []))
                
            if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in summary:
                for point in summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                    if point.get("é‡è¦åº¦", 0) >= 4:  # Only include high importance points
                        context["key_themes"].append({
                            "topic": point["ã‚¿ã‚¤ãƒˆãƒ«"],
                            "importance": point["é‡è¦åº¦"]
                        })
                        
            # Extract importance factors based on keyword frequency and context
            if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in summary:
                for keyword in summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                    context["importance_factors"].append({
                        "term": keyword["ç”¨èª"],
                        "weight": keyword.get("é‡è¦åº¦", 1)
                    })
        
        # Remove duplicates and normalize weights
        context["continuing_themes"] = list(set(context["continuing_themes"]))
        context["importance_factors"] = self._normalize_importance_weights(context["importance_factors"])
        return context

    def _normalize_importance_weights(self, factors: list) -> list:
        """é‡è¦åº¦ã®é‡ã¿ã‚’æ­£è¦åŒ–"""
        if not factors:
            return []
            
        # Calculate total weight
        total_weight = sum(factor["weight"] for factor in factors)
        
        # Normalize weights
        if total_weight > 0:
            return [{
                "term": factor["term"],
                "weight": factor["weight"] / total_weight
            } for factor in factors]
        return factors

    def _create_summary_prompt(self, chunk: str, context: dict) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸè¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        prompt = f'''
        ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚æ–‡è„ˆã‚’è€ƒæ…®ã—ã€å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã®é–¢é€£æ€§ã‚’ç¶­æŒã—ãªãŒã‚‰ã€
        å…ƒã®æ–‡ç« ã®30%ç¨‹åº¦ã®é•·ã•ã«æŠ‘ãˆã€ç°¡æ½”ã«é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®æ–‡è„ˆæƒ…å ±ï¼š
        - ç¶™ç¶šä¸­ã®ãƒˆãƒ”ãƒƒã‚¯: {", ".join(context.get("continuing_themes", []))}
        - ä¸»è¦ãƒ†ãƒ¼ãƒ: {json.dumps([theme["topic"] for theme in context.get("key_themes", [])[:3]], ensure_ascii=False)}
        - é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {json.dumps([{
            "term": factor["term"],
            "weight": f"{factor['weight']:.2f}"
        } for factor in context.get("importance_factors", [])[:5]], ensure_ascii=False)}
        
        ãƒ†ã‚­ã‚¹ãƒˆ:
        {chunk}
        
        å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
        {{
            "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [
                {{
                    "ã‚¿ã‚¤ãƒˆãƒ«": "ç°¡æ½”ãªãƒˆãƒ”ãƒƒã‚¯",
                    "èª¬æ˜": "30æ–‡å­—ä»¥å†…ã®èª¬æ˜",
                    "é‡è¦åº¦": 1-5ã®æ•°å€¤,
                    "æ–‡è„ˆé–¢é€£åº¦": 0-1ã®æ•°å€¤
                }}
            ],
            "è©³ç´°åˆ†æ": [
                {{
                    "ã‚»ã‚¯ã‚·ãƒ§ãƒ³": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
                    "ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ": ["é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆå„15æ–‡å­—ä»¥å†…ï¼‰"],
                    "å‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã®é–¢é€£": ["é–¢é€£ã™ã‚‹ãƒã‚¤ãƒ³ãƒˆ"]
                }}
            ],
            "æ–‡è„ˆé€£æº": {{
                "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯": ["ãƒˆãƒ”ãƒƒã‚¯å"],
                "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯": ["æ–°ãƒˆãƒ”ãƒƒã‚¯å"],
                "ãƒˆãƒ”ãƒƒã‚¯é–¢é€£åº¦": 0-1ã®æ•°å€¤
            }},
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [
                {{
                    "ç”¨èª": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                    "èª¬æ˜": "20æ–‡å­—ä»¥å†…ã®èª¬æ˜",
                    "é‡è¦åº¦": 1-5ã®æ•°å€¤
                }}
            ]
        }}
        '''
        return prompt

    def _format_summaries(self, summaries: list) -> str:
        """è¦ç´„ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        try:
            formatted_text = []
            
            for i, summary in enumerate(summaries, 1):
                formatted_text.append(f"## ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {i}\n")
                
                # Add main points with context relevance
                formatted_text.append("### ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ")
                for point in summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                    importance = "ğŸ”¥" * point.get("é‡è¦åº¦", 1)
                    context_relevance = f"(æ–‡è„ˆé–¢é€£åº¦: {point.get('æ–‡è„ˆé–¢é€£åº¦', 0):.1f})"
                    formatted_text.append(f"- {point['ã‚¿ã‚¤ãƒˆãƒ«']} {importance} {context_relevance}")
                    if "èª¬æ˜" in point:
                        formatted_text.append(f"  - {point['èª¬æ˜']}")
                        
                # Add detailed analysis with context connections
                formatted_text.append("\n### è©³ç´°åˆ†æ")
                for analysis in summary["è©³ç´°åˆ†æ"]:
                    formatted_text.append(f"#### {analysis['ã‚»ã‚¯ã‚·ãƒ§ãƒ³']}")
                    for point in analysis.get("ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ", []):
                        formatted_text.append(f"- {point}")
                    if "å‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã®é–¢é€£" in analysis:
                        formatted_text.append("\n*å‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã®é–¢é€£:*")
                        for relation in analysis["å‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã®é–¢é€£"]:
                            formatted_text.append(f"- â†’ {relation}")
                        
                # Add context linkage information
                if "æ–‡è„ˆé€£æº" in summary:
                    formatted_text.append("\n### æ–‡è„ˆé€£æº")
                    context_info = summary["æ–‡è„ˆé€£æº"]
                    if "ãƒˆãƒ”ãƒƒã‚¯é–¢é€£åº¦" in context_info:
                        formatted_text.append(f"*ãƒˆãƒ”ãƒƒã‚¯é–¢é€£åº¦: {context_info['ãƒˆãƒ”ãƒƒã‚¯é–¢é€£åº¦']:.1f}*")
                    if context_info.get("ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"):
                        formatted_text.append("\n**ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯:**")
                        for topic in context_info["ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"]:
                            formatted_text.append(f"- {topic}")
                    if context_info.get("æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"):
                        formatted_text.append("\n**æ–°è¦ãƒˆãƒ”ãƒƒã‚¯:**")
                        for topic in context_info["æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"]:
                            formatted_text.append(f"- {topic}")
                            
                # Add keywords with importance
                formatted_text.append("\n### ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
                for keyword in summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                    importance = "â­" * keyword.get("é‡è¦åº¦", 1)
                    formatted_text.append(f"- **{keyword['ç”¨èª']}** {importance}: {keyword['èª¬æ˜']}")
                    
                formatted_text.append("\n---\n")
                
            return "\n".join(formatted_text)
            
        except Exception as e:
            logger.error(f"Error formatting summaries: {str(e)}")
            return "è¦ç´„ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

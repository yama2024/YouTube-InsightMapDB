import os
import google.generativeai as genai
import logging
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import NoTranscriptFound, TranscriptsDisabled
import json
import re

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TextProcessor:
    def __init__(self):
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("Gemini API key is not set in environment variables")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.0-pro')
        self._cache = {}

    def _create_summary_prompt(self, text):
        prompt = f"""
ã‚ãªãŸã¯æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸè¦ç´„ç”Ÿæˆã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€æ–‡è„ˆã®æµã‚Œã¨é‡è¦åº¦ã‚’è€ƒæ…®ã—ãŸè¦ç´„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›è¦ä»¶:
1. å„ãƒˆãƒ”ãƒƒã‚¯ã®é–¢é€£æ€§ã¨é‡è¦åº¦ã‚’è€ƒæ…®
2. æ–‡è„ˆã®æµã‚Œã‚’ä¿æŒã—ãŸè‡ªç„¶ãªè¦ç´„
3. é‡è¦ãªã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æŠ½å‡ºã¨èª¬æ˜

å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
{{
    "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [
        {{
            "ã‚¿ã‚¤ãƒˆãƒ«": "ç°¡æ½”ãªãƒˆãƒ”ãƒƒã‚¯",
            "èª¬æ˜": "30æ–‡å­—ä»¥å†…ã®èª¬æ˜",
            "é‡è¦åº¦": 1-5ã®æ•°å€¤
        }}
    ],
    "è©³ç´°åˆ†æ": [
        {{
            "ã‚»ã‚¯ã‚·ãƒ§ãƒ³": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
            "ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ": ["é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆå„15æ–‡å­—ä»¥å†…ï¼‰"]
        }}
    ],
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [
        {{
            "ç”¨èª": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            "èª¬æ˜": "20æ–‡å­—ä»¥å†…ã®èª¬æ˜"
        }}
    ],
    "æ–‡è„ˆé€£æº": {{
        "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯": ["ãƒˆãƒ”ãƒƒã‚¯å"],
        "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯": ["æ–°ãƒˆãƒ”ãƒƒã‚¯å"]
    }}
}}

è€ƒæ…®ã™ã¹ããƒã‚¤ãƒ³ãƒˆ:
1. ãƒˆãƒ”ãƒƒã‚¯é–“ã®é–¢é€£æ€§ã‚’æ˜ç¢ºã«ç¤ºã™
2. é‡è¦åº¦ã®åˆ¤å®šåŸºæº–ã‚’æ–‡è„ˆã‹ã‚‰å°å‡º
3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ–‡è„ˆä¸Šã®å½¹å‰²ã‚’è€ƒæ…®
4. ãƒˆãƒ”ãƒƒã‚¯ã®ç¶™ç¶šæ€§ã¨æ–°è¦æ€§ã‚’åŒºåˆ¥

åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ:
{text}
"""
        return prompt

    def get_transcript(self, video_url):
        """Get transcript from YouTube video"""
        try:
            video_id = self._extract_video_id(video_url)
            
            # Check cache first
            cache_key = f"transcript_{video_id}"
            if cache_key in self._cache:
                return self._cache[cache_key]
            
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            transcript = transcript_list.find_transcript(['ja', 'en'])
            transcript_pieces = transcript.fetch()
            
            # Improved transcript processing with context preservation
            processed_pieces = []
            current_context = ""
            
            for piece in transcript_pieces:
                text = piece['text'].strip()
                # Preserve context between transcript pieces
                if text.endswith(('ã€', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
                    current_context += text + ' '
                    processed_pieces.append(current_context.strip())
                    current_context = ""
                else:
                    current_context += text + ' '
            
            if current_context:  # Add any remaining context
                processed_pieces.append(current_context.strip())
            
            full_transcript = ' '.join(processed_pieces)
            
            # Cache the result
            self._cache[cache_key] = full_transcript
            return full_transcript
            
        except (NoTranscriptFound, TranscriptsDisabled) as e:
            logger.error(f"å­—å¹•ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            raise Exception("ã“ã®å‹•ç”»ã§ã¯å­—å¹•ã‚’åˆ©ç”¨ã§ãã¾ã›ã‚“")
        except Exception as e:
            logger.error(f"å­—å¹•ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise Exception(f"å­—å¹•ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    def _extract_video_id(self, url):
        """Extract video ID from YouTube URL"""
        patterns = [
            r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
            r'(?:embed\/)([0-9A-Za-z_-]{11})',
            r'(?:watch\?v=)([0-9A-Za-z_-]{11})'
        ]
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        raise ValueError("ç„¡åŠ¹ãªYouTube URLã§ã™")

    def generate_summary(self, text):
        """Generate context-aware summary using Gemini"""
        try:
            # Check cache first
            cache_key = hash(text)
            if cache_key in self._cache:
                return self._cache[cache_key]

            # Split text into contextual chunks for better processing
            chunks = self._split_into_contextual_chunks(text)
            
            # Process each chunk while maintaining context
            summaries = []
            context = {}
            
            for chunk in chunks:
                prompt = self._create_summary_prompt(chunk)
                response = self.model.generate_content(prompt)
                
                if not response.text:
                    raise Exception("è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

                # Extract JSON from response
                json_str = response.text
                if json_str.startswith('```json'):
                    json_str = json_str[7:]
                if json_str.endswith('```'):
                    json_str = json_str[:-3]

                # Parse JSON and update context
                chunk_data = json.loads(json_str)
                self._update_context(context, chunk_data)
                summaries.append(chunk_data)
            
            # Merge summaries with context awareness
            merged_summary = self._merge_summaries(summaries, context)
            formatted_summary = self._format_summary(merged_summary)
            
            # Cache the result
            self._cache[cache_key] = formatted_summary
            return formatted_summary
            
        except Exception as e:
            logger.error(f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise Exception(f"è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    def _split_into_contextual_chunks(self, text, chunk_size=1000):
        """Split text into chunks while preserving context"""
        chunks = []
        sentences = re.split('([ã€‚ï¼ï¼Ÿ])', text)
        current_chunk = ""
        
        for i in range(0, len(sentences), 2):
            sentence = sentences[i] + (sentences[i + 1] if i + 1 < len(sentences) else "")
            if len(current_chunk) + len(sentence) > chunk_size:
                chunks.append(current_chunk)
                current_chunk = sentence
            else:
                current_chunk += sentence
                
        if current_chunk:
            chunks.append(current_chunk)
            
        return chunks

    def _update_context(self, context, chunk_data):
        """Update context information from chunk data"""
        # Update continuing topics
        if "æ–‡è„ˆé€£æº" in chunk_data:
            if "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯" not in context:
                context["ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"] = set()
            if "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯" not in context:
                context["æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"] = set()
                
            context["ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"].update(chunk_data["æ–‡è„ˆé€£æº"]["ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"])
            context["æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"].update(chunk_data["æ–‡è„ˆé€£æº"]["æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"])

    def _merge_summaries(self, summaries, context):
        """Merge chunk summaries with context awareness"""
        merged = {
            "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [],
            "è©³ç´°åˆ†æ": [],
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [],
            "æ–‡è„ˆé€£æº": {
                "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯": list(context.get("ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯", [])),
                "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯": list(context.get("æ–°è¦ãƒˆãƒ”ãƒƒã‚¯", []))
            }
        }
        
        # Merge while maintaining importance and context
        seen_topics = set()
        for summary in summaries:
            # Merge main points with deduplication
            for point in summary.get("ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ", []):
                if point["ã‚¿ã‚¤ãƒˆãƒ«"] not in seen_topics:
                    merged["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"].append(point)
                    seen_topics.add(point["ã‚¿ã‚¤ãƒˆãƒ«"])
            
            # Merge detailed analysis
            merged["è©³ç´°åˆ†æ"].extend(summary.get("è©³ç´°åˆ†æ", []))
            
            # Merge keywords with deduplication
            for keyword in summary.get("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", []):
                if not any(k["ç”¨èª"] == keyword["ç”¨èª"] for k in merged["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]):
                    merged["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].append(keyword)

        return merged

    def _format_summary(self, data):
        """Format the summary data into a readable markdown string"""
        try:
            sections = []
            
            # Main points section
            if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in data:
                sections.append("## ğŸ“Œ ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ\n")
                for point in data["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                    importance = "ğŸ”¥" * point.get("é‡è¦åº¦", 1)
                    sections.append(f"### {point['ã‚¿ã‚¤ãƒˆãƒ«']} {importance}\n")
                    sections.append(f"{point['èª¬æ˜']}\n")

            # Detailed analysis section
            if "è©³ç´°åˆ†æ" in data:
                sections.append("\n## ğŸ“Š è©³ç´°åˆ†æ\n")
                for analysis in data["è©³ç´°åˆ†æ"]:
                    sections.append(f"### {analysis['ã‚»ã‚¯ã‚·ãƒ§ãƒ³']}\n")
                    for point in analysis['ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ']:
                        sections.append(f"- {point}\n")

            # Keywords section
            if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in data:
                sections.append("\n## ğŸ” é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\n")
                for keyword in data["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                    sections.append(f"**{keyword['ç”¨èª']}**: {keyword['èª¬æ˜']}\n")

            # Context connection section
            if "æ–‡è„ˆé€£æº" in data:
                sections.append("\n## ğŸ”„ æ–‡è„ˆã®é€£æº\n")
                
                if data["æ–‡è„ˆé€£æº"]["ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"]:
                    sections.append("### ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯\n")
                    for topic in data["æ–‡è„ˆé€£æº"]["ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯"]:
                        sections.append(f"- {topic}\n")
                
                if data["æ–‡è„ˆé€£æº"]["æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"]:
                    sections.append("\n### æ–°è¦ãƒˆãƒ”ãƒƒã‚¯\n")
                    for topic in data["æ–‡è„ˆé€£æº"]["æ–°è¦ãƒˆãƒ”ãƒƒã‚¯"]:
                        sections.append(f"- {topic}\n")

            return "\n".join(sections)
            
        except Exception as e:
            logger.error(f"è¦ç´„ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return "è¦ç´„ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ"

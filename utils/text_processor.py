import logging
from youtube_transcript_api import YouTubeTranscriptApi
import google.generativeai as genai
import re
import os
import time
from typing import List, Optional, Dict, Any, Tuple
from retrying import retry
from cachetools import TTLCache, cached

# Enhanced logging setup
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TextProcessor:
    def __init__(self):
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("Gemini API key is not set in environment variables")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-pro')
        
        # Initialize caches
        self.subtitle_cache = TTLCache(maxsize=100, ttl=3600)  # 1 hour TTL
        self.processed_text_cache = TTLCache(maxsize=100, ttl=1800)  # 30 minutes TTL
        
        # Enhanced noise patterns for Japanese text
        self.noise_patterns = {
            'timestamps': r'\[?\(?\d{1,2}:\d{2}(?::\d{2})?\]?\)?',
            'speaker_tags': r'\[[^\]]*\]|\([^)]*\)',
            'filler_words': r'\b(ãˆãƒ¼ã¨|ãˆã£ã¨|ãˆãƒ¼|ã‚ã®|ã‚ã®ãƒ¼|ã¾ã|ã‚“ãƒ¼|ãã®ãƒ¼|ãªã‚“ã‹|ã“ã†|ã­|ã­ã‡|ã•ã|ã†ãƒ¼ã‚“|ã‚ãƒ¼|ãã†ã§ã™ã­|ã¡ã‚‡ã£ã¨)\b',
            'repeated_chars': r'([^\W\d_])\1{3,}',
            'multiple_spaces': r'[\sã€€]{2,}',
            'empty_lines': r'\n\s*\n',
            'punctuation': r'([ã€‚ï¼ï¼ï¼Ÿ])\1+',
            'noise_symbols': r'[â™ªâ™«â™¬â™©â€ â€¡â—Šâ—†â—‡â– â–¡â–²â–³â–¼â–½â—‹â—â—]',
            'parentheses': r'ï¼ˆ[^ï¼‰]*ï¼‰|\([^)]*\)',
            'unnecessary_symbols': r'[ï¼Šâˆ—â€»#ï¼ƒâ˜…â˜†â–ºâ–·â—â—€â†’â†â†‘â†“]',
            'commercial_markers': r'(?:CM|åºƒå‘Š|ã‚¹ãƒãƒ³ã‚µãƒ¼)(?:\s*\d*)?',
            'system_messages': r'(?:ã‚·ã‚¹ãƒ†ãƒ |ã‚¨ãƒ©ãƒ¼|é€šçŸ¥)(?:ï¼š|:).*?(?:\n|$)',
            'automated_tags': r'\[(?:éŸ³æ¥½|æ‹æ‰‹|ç¬‘|BGM|SE|åŠ¹æœéŸ³)\]'
        }
        
        # Japanese text normalization
        self.jp_normalization = {
            'spaces': {
                'ã€€': ' ',
                '\u3000': ' ',
                '\xa0': ' '
            },
            'punctuation': {
                'ï¼': 'ã€‚',
                'â€¦': 'ã€‚',
                '.': 'ã€‚',
                'ï½¡': 'ã€‚',
                'ï½¤': 'ã€'
            }
        }

    def _clean_text(self, text: str, progress_callback=None) -> str:
        """Enhanced text cleaning with progress tracking"""
        if not text:
            return ""
        
        try:
            total_steps = len(self.jp_normalization) + len(self.noise_patterns) + 1
            current_step = 0
            
            # Normalize Japanese text
            for category, replacements in self.jp_normalization.items():
                for old, new in replacements.items():
                    text = text.replace(old, new)
                current_step += 1
                if progress_callback:
                    progress_callback(current_step / total_steps, f"æ­£è¦åŒ–å‡¦ç†ä¸­: {category}")
            
            # Remove noise patterns
            for pattern_name, pattern in self.noise_patterns.items():
                text = re.sub(pattern, '', text)
                current_step += 1
                if progress_callback:
                    progress_callback(current_step / total_steps, f"ãƒã‚¤ã‚ºé™¤å»ä¸­: {pattern_name}")
            
            # Improve sentence structure
            text = self._improve_sentence_structure(text)
            current_step += 1
            if progress_callback:
                progress_callback(1.0, "æ–‡ç« æ§‹é€ ã®æœ€é©åŒ–å®Œäº†")
            
            return text
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return text

    def _chunk_text(self, text: str, chunk_size: int = 1500, overlap: int = 200) -> List[str]:
        if not text:
            return []
            
        # Split into sentences first
        sentences = re.split('([ã€‚!?ï¼ï¼Ÿ]+)', text)
        sentences = [''.join(i) for i in zip(sentences[0::2], sentences[1::2])]
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            if current_length + sentence_length > chunk_size:
                if current_chunk:
                    chunks.append(''.join(current_chunk))
                    # Keep last sentence for overlap
                    current_chunk = current_chunk[-1:] if overlap > 0 else []
                    current_length = sum(len(s) for s in current_chunk)
                
            current_chunk.append(sentence)
            current_length += sentence_length
            
        if current_chunk:
            chunks.append(''.join(current_chunk))
            
        return chunks

    def generate_summary(self, text: str) -> str:
        """Generate summary with improved chunking and error handling"""
        try:
            # Smaller chunks for better processing
            chunk_size = 800  # Reduced from 1500
            overlap = 100
            chunks = self._chunk_text(text, chunk_size, overlap)
            
            summaries = []
            for i, chunk in enumerate(chunks):
                try:
                    # Add delay between chunks
                    if i > 0:
                        time.sleep(2)
                    
                    prompt = f'''
                    ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼š
                    
                    {chunk}
                    
                    ãƒã‚¤ãƒ³ãƒˆï¼š
                    - é‡è¦ãªæƒ…å ±ã‚’ä¿æŒ
                    - ç°¡æ½”ã«è¡¨ç¾
                    - æ–‡è„ˆã‚’ç¶­æŒ
                    '''
                    
                    # Improved retry logic
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {i+1}/{len(chunks)} ã®å‡¦ç†ã‚’è©¦è¡Œä¸­ (è©¦è¡Œ {attempt+1}/{max_retries})")
                            response = self.model.generate_content(
                                prompt,
                                generation_config=genai.types.GenerationConfig(
                                    temperature=0.3,
                                    top_p=0.8,
                                    top_k=40,
                                    max_output_tokens=1024,
                                )
                            )
                            
                            if response and response.text:
                                summaries.append(response.text.strip())
                                logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {i+1} ã®è¦ç´„ãŒæˆåŠŸã—ã¾ã—ãŸ")
                                break
                            
                        except Exception as e:
                            logger.warning(f"ãƒãƒ£ãƒ³ã‚¯ {i+1} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ (è©¦è¡Œ {attempt+1}): {str(e)}")
                            if attempt < max_retries - 1:
                                wait_time = (2 ** attempt) + 1  # Exponential backoff
                                logger.info(f"å†è©¦è¡Œã¾ã§ {wait_time} ç§’å¾…æ©Ÿã—ã¾ã™")
                                time.sleep(wait_time)
                                continue
                            raise
                            
                except Exception as e:
                    logger.error(f"ãƒãƒ£ãƒ³ã‚¯ {i+1} ã®å‡¦ç†ã«å¤±æ•—: {str(e)}")
                    continue

            if not summaries:
                logger.error("è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—: æœ‰åŠ¹ãªè¦ç´„ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                raise ValueError("è¦ç´„ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            # Combine summaries with better formatting
            logger.info("å€‹åˆ¥ã®è¦ç´„ã‚’çµåˆã—ã¦æœ€çµ‚è¦ç´„ã‚’ç”Ÿæˆã—ã¾ã™")
            combined = "\n\n".join(summaries)
            final_prompt = f'''
            ä»¥ä¸‹ã®è¦ç´„ã‚’ã•ã‚‰ã«æ•´ç†ã—ã¦ã€ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š
            
            {combined}
            '''
            
            # Final summary with retry
            for attempt in range(3):
                try:
                    logger.info(f"æœ€çµ‚è¦ç´„ã®ç”Ÿæˆã‚’è©¦è¡Œä¸­ (è©¦è¡Œ {attempt+1}/3)")
                    final_response = self.model.generate_content(
                        final_prompt,
                        generation_config=genai.types.GenerationConfig(
                            temperature=0.3,
                            top_p=0.8,
                            top_k=40,
                            max_output_tokens=1024,
                        )
                    )
                    if final_response and final_response.text:
                        logger.info("æœ€çµ‚è¦ç´„ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
                        return final_response.text.strip()
                except Exception as e:
                    logger.warning(f"æœ€çµ‚è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ (è©¦è¡Œ {attempt+1}): {str(e)}")
                    if attempt < 2:
                        wait_time = 2 ** attempt
                        logger.info(f"å†è©¦è¡Œã¾ã§ {wait_time} ç§’å¾…æ©Ÿã—ã¾ã™")
                        time.sleep(wait_time)
                        continue
                    raise

            logger.error("æœ€çµ‚è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            raise ValueError("æœ€çµ‚è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise Exception(f"è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    def proofread_text(self, text: str, progress_callback=None) -> str:
        """Proofread and enhance text readability with progress tracking"""
        if not text:
            return ""
            
        try:
            if progress_callback:
                progress_callback(0.1, "ğŸ” ãƒ†ã‚­ã‚¹ãƒˆè§£æã‚’é–‹å§‹")
            
            # Initial text cleaning with detailed progress
            cleaning_steps = {
                0.15: "ğŸ“ ãƒ•ã‚£ãƒ©ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å»ä¸­...",
                0.20: "ğŸ”¤ æ–‡å­—ã®æ­£è¦åŒ–ã‚’å®Ÿè¡Œä¸­...",
                0.25: "ğŸ“Š ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å‡¦ç†ä¸­...",
                0.30: "âœ¨ ä¸è¦ãªè¨˜å·ã‚’å‰Šé™¤ä¸­..."
            }
            
            for progress, message in cleaning_steps.items():
                if progress_callback:
                    progress_callback(progress, message)
                time.sleep(0.3)  # Visual feedback
            
            text = self._clean_text(text, lambda p, m: progress_callback(0.3 + p * 0.2, m) if progress_callback else None)
            
            if progress_callback:
                progress_callback(0.5, "ğŸ¤– AIãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ–‡ç« æ ¡æ­£ã‚’æº–å‚™ä¸­...")
            
            # AI Processing steps
            prompt = f"""
# ã‚ãªãŸã®ç›®çš„:
ã€ŒOriginal Transcriptã€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¨æ–‡æ ¡é–²ã—ã¾ã™ã€‚

æ–‡å­—èµ·ã“ã—ã—ãŸYouTubeã®å‹•ç”»ã«ã¤ã„ã¦ã€å…ƒã®æ–‡ç« ã®æ„å‘³ã‚’çµ¶å¯¾ã«å¤‰æ›´ã›ãšã«æ–‡å­—èµ·ã“ã—ã¨æ ¡é–²ã‚’è¡Œã„ã¾ã™ã€‚
ã‚ãªãŸãŒæ–‡è„ˆã¨ã—ã¦ä¸è‡ªç„¶ã¨æ„Ÿã˜ãŸæ–‡ç« ã¯å…¨ã¦èª¤å­—è„±å­—ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€æ­£ç¢ºã«ä¿®æ­£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§æ€è€ƒã—ãªãŒã‚‰æ ¡é–²ã‚’è¡Œã„ã€æ­£ç¢ºã«ä¿®æ­£ã—ã¦æ–‡ç« ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

# ãƒ«ãƒ¼ãƒ«:
1.æ ¡é–²ã—ãŸæ–‡ç« ä»¥å¤–ã®å‡ºåŠ›ã¯æ±ºã—ã¦è¡Œã£ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
2.æ ¡é–²ã—ãŸæ–‡ç« ã®ã¿ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚
3.æ”¹è¡Œã®ä½ç½®ãŒä¸è‡ªç„¶ã ã£ãŸå ´åˆã¯æ–‡ç« ã¨å…±ã«é©åˆ‡ã«æ”¹è¡Œä½ç½®ã‚‚ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
4.æ™‚é–“ã‚’æ„å‘³ã™ã‚‹ã‚ˆã†ãªè¡¨ç¤ºã¨ã—ã¦"(00:00)"ã¨ã„ã£ãŸè¨˜è¼‰ãŒã‚ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ãŒã€ãã‚Œã¯æ–‡ç« ã§ã¯ãªã„ã®ã§ã€æ–‡ç« ã‹ã‚‰å‰Šé™¤ã—ã¦æ ¡é–²ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
5.ã‚¹ãƒ”ãƒ¼ãƒtoãƒ†ã‚­ã‚¹ãƒˆã§æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ã„ã‚‹å ´åˆã€ã€Œãˆãƒ¼ã€ã€ã€Œã¾ã‚ã€ã€ã€Œã‚ã®ãƒ¼ã€ã¨ã„ã£ãŸãƒ•ã‚£ãƒ©ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã“ã¡ã‚‰ã‚‚å‰Šé™¤ã—ã¦æ ¡é–²ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
6.ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›ã™ã‚‹ã¨ãã«ã¯ã€ã€Œã€‚ã€ã§æ”¹è¡Œã‚’è¡Œã£ã¦è¦‹ã‚„ã™ã„æ–‡ç« ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼š
{text}
"""
            
            if progress_callback:
                progress_callback(0.6, "ğŸ§  AIã«ã‚ˆã‚‹æ–‡ç« è§£æä¸­...")
                time.sleep(0.3)
                progress_callback(0.7, "ğŸ“ æ–‡ç« ã®æ ¡æ­£ã‚’å®Ÿè¡Œä¸­...")
            
            response = self.model.generate_content(prompt)
            if not response.text:
                logger.error("AIãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸ")
                if progress_callback:
                    progress_callback(1.0, "âŒ ã‚¨ãƒ©ãƒ¼: AIãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã™")
                return text
            
            if progress_callback:
                progress_callback(0.8, "ğŸ¨ æ–‡ç« ã®æœ€çµ‚èª¿æ•´ä¸­...")
            
            enhanced_text = response.text
            enhanced_text = self._clean_text(enhanced_text)
            
            if progress_callback:
                progress_callback(0.9, "ğŸ“Š æ–‡ç« æ§‹é€ ã‚’æœ€é©åŒ–ä¸­...")
            
            enhanced_text = self._improve_sentence_structure(enhanced_text)
            enhanced_text = re.sub(r'([ã€‚])', r'\1\n', enhanced_text)
            enhanced_text = re.sub(r'\n{3,}', '\n\n', enhanced_text)
            enhanced_text = enhanced_text.strip()
            
            if progress_callback:
                progress_callback(1.0, "âœ¨ æ ¡æ­£å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!")
            
            return enhanced_text
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆã®æ ¡æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            if progress_callback:
                progress_callback(1.0, f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return text

    def _improve_sentence_structure(self, text: str) -> str:
        """Improve Japanese sentence structure and readability"""
        try:
            # Fix sentence endings
            text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*(?=[^ã€ã€ï¼‰])', r'\1\n', text)
            
            # Improve paragraph breaks
            text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*\n\s*([^ã€Œã€ï¼ˆ])', r'\1\n\n\2', text)
            
            # Fix spacing around Japanese punctuation
            text = re.sub(r'\s+([ã€‚ã€ï¼ï¼Ÿã€ã€ï¼‰])', r'\1', text)
            text = re.sub(r'([ã€Œã€ï¼ˆ])\s+', r'\1', text)
            
            # Clean up list items
            text = re.sub(r'^[-ãƒ»]\s*', 'â€¢ ', text, flags=re.MULTILINE)
            
            return text
        except Exception as e:
            logger.error(f"æ–‡ç« æ§‹é€ ã®æ”¹å–„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return text

    @retry(stop_max_attempt_number=3, wait_fixed=2000)
    def get_transcript(self, url: str) -> str:
        """Get transcript with improved error handling and retries"""
        video_id = self._extract_video_id(url)
        if not video_id:
            raise ValueError("ç„¡åŠ¹ãªYouTube URLã§ã™")
        
        try:
            # Check cache first
            cached_transcript = self.subtitle_cache.get(video_id)
            if cached_transcript:
                logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå­—å¹•ã‚’ä½¿ç”¨ã—ã¾ã™")
                return cached_transcript
            
            transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ja', 'ja-JP', 'en'])
            transcript = ' '.join([entry['text'] for entry in transcript_list])
            
            cleaned_transcript = self._clean_text(transcript)
            self.subtitle_cache[video_id] = cleaned_transcript
            return cleaned_transcript
            
        except Exception as e:
            error_msg = f"å­—å¹•å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(error_msg)
            raise ValueError(error_msg)

    def _extract_video_id(self, url: str) -> Optional[str]:
        """Extract video ID from YouTube URL"""
        try:
            patterns = [
                r'(?:v=|/v/|youtu\.be/)([^&?/]+)',
                r'(?:embed/|v/)([^/?]+)',
                r'(?:watch\?v=|/v/|youtu\.be/)([^&?/]+)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    return match.group(1)
            return None
            
        except Exception as e:
            logger.error(f"å‹•ç”»IDæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None

    def generate_summary(self, text: str) -> str:
        """Generate summary with improved error handling"""
        if not text:
            return ""
            
        try:
            prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ç®‡æ¡æ›¸ãã§ç¤ºã—ã€
            ãã®å¾Œã«ç°¡æ½”ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

            {text}

            å‡ºåŠ›å½¢å¼ï¼š
            â–  ä¸»ãªãƒã‚¤ãƒ³ãƒˆï¼š
            â€¢ ãƒã‚¤ãƒ³ãƒˆ1
            â€¢ ãƒã‚¤ãƒ³ãƒˆ2
            â€¢ ãƒã‚¤ãƒ³ãƒˆ3

            â–  è¦ç´„ï¼š
            [ç°¡æ½”ãªè¦ç´„æ–‡]
            """
            
            response = self.model.generate_content(prompt)
            return response.text if response.text else "è¦ç´„ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            
        except Exception as e:
            logger.error(f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return "è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
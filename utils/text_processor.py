import os
import json
import logging
from typing import Dict, List
import google.generativeai as genai
import re
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TextProcessor:
    def __init__(self, max_workers=3):
        self._cache = {}
        self.max_workers = max_workers
        # Initialize Gemini
        genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
        self.model = genai.GenerativeModel('gemini-pro')

    def get_transcript(self, video_url: str) -> str:
        """å‹•ç”»ã®æ–‡å­—èµ·ã“ã—ã‚’å–å¾—"""
        try:
            video_id = self._extract_video_id(video_url)
            
            # Check cache first
            cache_key = f"transcript_{video_id}"
            if cache_key in self._cache:
                return self._cache[cache_key]
            
            transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
            transcript = transcript_list.find_transcript(['ja', 'en'])
            transcript_data = transcript.fetch()
            
            formatter = TextFormatter()
            formatted_transcript = formatter.format_transcript(transcript_data)
            
            # Cache the result
            self._cache[cache_key] = formatted_transcript
            return formatted_transcript
            
        except Exception as e:
            logger.error(f"æ–‡å­—èµ·ã“ã—ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise Exception(f"æ–‡å­—èµ·ã“ã—ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    def _extract_video_id(self, url: str) -> str:
        """YouTube URLã‹ã‚‰ãƒ“ãƒ‡ã‚ªIDã‚’æŠ½å‡º"""
        patterns = [
            r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
            r'(?:embed\/)([0-9A-Za-z_-]{11})',
            r'(?:watch\?v=)([0-9A-Za-z_-]{11})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        raise ValueError("ç„¡åŠ¹ãªYouTube URLã§ã™")

    def _split_into_contextual_chunks(self, text: str, chunk_size: int = 500) -> List[str]:
        sentences = re.split('([ã€‚!?ï¼ï¼Ÿ]+)', text)
        sentences = [''.join(i) for i in zip(sentences[0::2], sentences[1::2])]
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            if current_length + len(sentence) > chunk_size and current_chunk:
                chunks.append(''.join(current_chunk))
                current_chunk = []
                current_length = 0
            current_chunk.append(sentence)
            current_length += len(sentence)
        
        if current_chunk:
            chunks.append(''.join(current_chunk))
        
        return chunks

    def _process_chunk_with_retry(self, chunk: str, chunk_index: int) -> Dict:
        for attempt in range(3):
            try:
                logger.info(f"Processing chunk {chunk_index + 1}, attempt {attempt + 1}")
                response = self.model.generate_content(
                    self._create_summary_prompt(chunk),
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.3,
                        top_p=0.8,
                        top_k=40,
                        max_output_tokens=4096,
                    )
                )
                
                if not response or not response.text:
                    logger.warning(f"Empty response for chunk {chunk_index + 1}")
                    continue
                
                logger.info(f"Raw response: {response.text[:200]}")
                data = self._validate_summary_response(response.text)
                
                if data:
                    return data
                
            except Exception as e:
                logger.error(f"Error processing chunk {chunk_index + 1}: {str(e)}")
                if attempt < 2:
                    time.sleep(2 ** attempt)
        
        return None

    def _create_summary_prompt(self, text: str) -> str:
        return f'''
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚
å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

{{
    "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [
        {{
            "ã‚¿ã‚¤ãƒˆãƒ«": "è¦ç‚¹",
            "èª¬æ˜": "è©³ç´°èª¬æ˜ï¼ˆ30æ–‡å­—ä»¥å†…ï¼‰",
            "é‡è¦åº¦": 3
        }}
    ],
    "è©³ç´°åˆ†æ": [
        {{
            "ã‚»ã‚¯ã‚·ãƒ§ãƒ³": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
            "ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ": [
                "é‡è¦ãƒã‚¤ãƒ³ãƒˆ1",
                "é‡è¦ãƒã‚¤ãƒ³ãƒˆ2"
            ]
        }}
    ],
    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [
        {{
            "ç”¨èª": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            "èª¬æ˜": "èª¬æ˜ï¼ˆ20æ–‡å­—ä»¥å†…ï¼‰"
        }}
    ]
}}

ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

é‡è¦:
1. å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã™ã‚‹ã“ã¨
2. é‡è¦åº¦ã¯1ã‹ã‚‰5ã®æ•´æ•°ã§ã‚ã‚‹ã“ã¨
3. èª¬æ˜ã¯æŒ‡å®šã•ã‚ŒãŸæ–‡å­—æ•°ä»¥å†…ã«åã‚ã‚‹ã“ã¨
'''

    def _validate_summary_response(self, response_text: str) -> dict:
        try:
            # Clean up response
            json_str = response_text.strip()
            if json_str.startswith('```json'):
                json_str = json_str[7:]
            if json_str.endswith('```'):
                json_str = json_str[:-3]
            
            # Try to extract JSON if embedded in text
            json_match = re.search(r'({[\s\S]*})', json_str)
            if json_match:
                json_str = json_match.group(1)
            
            # Parse JSON
            data = json.loads(json_str)
            
            # Create default structure if missing
            if not isinstance(data, dict):
                return None
                
            if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" not in data:
                data["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"] = []
            if "è©³ç´°åˆ†æ" not in data:
                data["è©³ç´°åˆ†æ"] = []
            if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" not in data:
                data["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"] = []
                
            # Ensure at least one main point
            if not data["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                data["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"] = [{
                    "ã‚¿ã‚¤ãƒˆãƒ«": "ãƒ†ã‚­ã‚¹ãƒˆæ¦‚è¦",
                    "èª¬æ˜": "ãƒ†ã‚­ã‚¹ãƒˆã®ä¸»è¦ãªå†…å®¹",
                    "é‡è¦åº¦": 3
                }]
                
            return data
            
        except Exception as e:
            logger.error(f"Response validation failed: {str(e)}\nResponse text: {response_text[:200]}...")
            return None

    def generate_summary(self, text: str) -> str:
        try:
            cache_key = hash(text)
            if cache_key in self._cache:
                return self._cache[cache_key]

            chunks = self._split_into_contextual_chunks(text)
            summaries = []
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_chunk = {
                    executor.submit(self._process_chunk_with_retry, chunk, i): i
                    for i, chunk in enumerate(chunks)
                }
                
                for future in as_completed(future_to_chunk):
                    chunk_index = future_to_chunk[future]
                    try:
                        summary = future.result()
                        if summary:
                            summaries.append(summary)
                            logger.info(f"Successfully completed chunk {chunk_index + 1}/{len(chunks)}")
                    except Exception as e:
                        logger.error(f"Failed to process chunk {chunk_index + 1}: {str(e)}")
            
            if not summaries:
                raise ValueError("æœ‰åŠ¹ãªè¦ç´„ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
            merged_summary = self._merge_summaries(summaries)
            formatted_summary = self._format_summary(merged_summary)
            
            self._cache[cache_key] = formatted_summary
            return formatted_summary
            
        except Exception as e:
            logger.error(f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise Exception(f"è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    def _merge_summaries(self, summaries: List[Dict]) -> Dict:
        """è¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯è¦ç´„ã‚’ãƒãƒ¼ã‚¸"""
        merged = {
            "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [],
            "è©³ç´°åˆ†æ": [],
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": []
        }
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        all_points = []
        for summary in summaries:
            if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in summary:
                all_points.extend(summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"])
        
        sorted_points = sorted(
            all_points,
            key=lambda x: x.get("é‡è¦åº¦", 0),
            reverse=True
        )
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¦ä¸Šä½ã‚’é¸æŠ
        seen_titles = set()
        for point in sorted_points:
            title = point["ã‚¿ã‚¤ãƒˆãƒ«"]
            if title not in seen_titles:
                seen_titles.add(title)
                merged["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"].append(point)

        # è©³ç´°åˆ†æã‚’ãƒãƒ¼ã‚¸
        seen_sections = set()
        for summary in summaries:
            if "è©³ç´°åˆ†æ" in summary:
                for analysis in summary["è©³ç´°åˆ†æ"]:
                    if analysis["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"] not in seen_sections:
                        seen_sections.add(analysis["ã‚»ã‚¯ã‚·ãƒ§ãƒ³"])
                        merged["è©³ç´°åˆ†æ"].append(analysis)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒãƒ¼ã‚¸
        seen_keywords = set()
        for summary in summaries:
            if "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" in summary:
                for keyword in summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                    if keyword["ç”¨èª"] not in seen_keywords:
                        seen_keywords.add(keyword["ç”¨èª"])
                        merged["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].append(keyword)
        
        return merged

    def _format_summary(self, merged_summary: Dict) -> str:
        """è¦ç´„ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        formatted_lines = ["# ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¦ç´„\n"]
        
        # ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ
        formatted_lines.append("## ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ")
        for point in merged_summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
            importance = "ğŸ”¥" * point.get("é‡è¦åº¦", 1)
            formatted_lines.append(
                f"\n### {point['ã‚¿ã‚¤ãƒˆãƒ«']} {importance}\n"
                f"{point['èª¬æ˜']}"
            )
        
        # è©³ç´°åˆ†æ
        if merged_summary["è©³ç´°åˆ†æ"]:
            formatted_lines.append("\n## è©³ç´°åˆ†æ")
            for analysis in merged_summary["è©³ç´°åˆ†æ"]:
                formatted_lines.append(f"\n### {analysis['ã‚»ã‚¯ã‚·ãƒ§ãƒ³']}")
                for point in analysis["ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ"]:
                    formatted_lines.append(f"- {point}")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        if merged_summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
            formatted_lines.append("\n## é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
            for keyword in merged_summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                formatted_lines.append(
                    f"\n- **{keyword['ç”¨èª']}**: {keyword['èª¬æ˜']}"
                )
        
        return "\n".join(formatted_lines)

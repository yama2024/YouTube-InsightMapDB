import os
import json
import google.generativeai as genai
from youtube_transcript_api import YouTubeTranscriptApi
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TextProcessor:
    def __init__(self):
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("Gemini API key is not set in environment variables")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        self._cache = {}

    def get_transcript(self, url):
        """YouTubeã®å­—å¹•ã‚’å–å¾—"""
        try:
            video_id = url.split("v=")[-1].split("&")[0]
            
            # Check cache first
            cache_key = f"transcript_{video_id}"
            if cache_key in self._cache:
                return self._cache[cache_key]

            transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ja', 'en'])
            transcript = ' '.join([entry['text'] for entry in transcript_list])
            
            # Cache the result
            self._cache[cache_key] = transcript
            return transcript
            
        except Exception as e:
            logger.error(f"å­—å¹•ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            raise Exception(f"å­—å¹•ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

    def _validate_json_response(self, response_text: str) -> dict:
        """JSON responseã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            # Clean up the response text
            cleaned_text = response_text.strip()
            if cleaned_text.startswith('```json'):
                cleaned_text = cleaned_text[7:]
            if cleaned_text.endswith('```'):
                cleaned_text = cleaned_text[:-3]
                
            # Try to parse JSON
            data = json.loads(cleaned_text)
            
            # Validate required fields
            required_fields = ["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ", "è©³ç´°åˆ†æ", "æ–‡è„ˆé€£æº", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]
            if not all(field in data for field in required_fields):
                logger.warning("Missing required fields in JSON response")
                return None
                
            return data
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON response: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"Error validating JSON response: {str(e)}")
            return None

    def _get_chunk_context(self, previous_summaries: list) -> dict:
        """å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡º"""
        context = {
            "continuing_themes": [],
            "key_themes": []
        }
        
        if not previous_summaries:
            return context
            
        # Get themes from previous summaries
        for summary in previous_summaries[-2:]:  # Look at last 2 summaries
            if "æ–‡è„ˆé€£æº" in summary:
                context["continuing_themes"].extend(summary["æ–‡è„ˆé€£æº"].get("ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯", []))
                
            if "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ" in summary:
                for point in summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                    if point.get("é‡è¦åº¦", 0) >= 4:  # Only include high importance points
                        context["key_themes"].append({
                            "topic": point["ã‚¿ã‚¤ãƒˆãƒ«"],
                            "importance": point["é‡è¦åº¦"]
                        })
        
        # Remove duplicates
        context["continuing_themes"] = list(set(context["continuing_themes"]))
        return context

    def _create_summary_prompt(self, chunk: str, context: dict) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸè¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        prompt = f'''
        ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚å…ƒã®æ–‡ç« ã®30%ç¨‹åº¦ã®é•·ã•ã«æŠ‘ãˆã€ç°¡æ½”ã«é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®æ–‡è„ˆæƒ…å ±ï¼š
        - ç¶™ç¶šä¸­ã®ãƒˆãƒ”ãƒƒã‚¯: {", ".join(context.get("continuing_themes", []))}
        - ä¸»è¦ãƒ†ãƒ¼ãƒ: {json.dumps([theme["topic"] for theme in context.get("key_themes", [])[:3]], ensure_ascii=False)}
        
        ãƒ†ã‚­ã‚¹ãƒˆ:
        {chunk}
        
        å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
        {{
            "ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ": [
                {{
                    "ã‚¿ã‚¤ãƒˆãƒ«": "ç°¡æ½”ãªãƒˆãƒ”ãƒƒã‚¯",
                    "èª¬æ˜": "30æ–‡å­—ä»¥å†…ã®èª¬æ˜",
                    "é‡è¦åº¦": 1-5ã®æ•°å€¤
                }}
            ],
            "è©³ç´°åˆ†æ": [
                {{
                    "ã‚»ã‚¯ã‚·ãƒ§ãƒ³": "ã‚»ã‚¯ã‚·ãƒ§ãƒ³å",
                    "ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ": ["é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆå„15æ–‡å­—ä»¥å†…ï¼‰"]
                }}
            ],
            "æ–‡è„ˆé€£æº": {{
                "ç¶™ç¶šã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯": ["ãƒˆãƒ”ãƒƒã‚¯å"],
                "æ–°è¦ãƒˆãƒ”ãƒƒã‚¯": ["æ–°ãƒˆãƒ”ãƒƒã‚¯å"]
            }},
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [
                {{
                    "ç”¨èª": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                    "èª¬æ˜": "20æ–‡å­—ä»¥å†…ã®èª¬æ˜"
                }}
            ]
        }}
        '''
        return prompt

    def _chunk_text(self, text: str, chunk_size: int = 1500) -> list:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        
        for word in words:
            word_length = len(word)
            if current_length + word_length + 1 <= chunk_size:
                current_chunk.append(word)
                current_length += word_length + 1
            else:
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_length = word_length
                
        if current_chunk:
            chunks.append(' '.join(current_chunk))
            
        return chunks

    def _format_summaries(self, summaries: list) -> str:
        """è¦ç´„ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        try:
            formatted_text = []
            
            for i, summary in enumerate(summaries, 1):
                formatted_text.append(f"## ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {i}\n")
                
                # Add main points
                formatted_text.append("### ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ")
                for point in summary["ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ"]:
                    importance = "ğŸ”¥" * point.get("é‡è¦åº¦", 1)
                    formatted_text.append(f"- {point['ã‚¿ã‚¤ãƒˆãƒ«']} {importance}")
                    if "èª¬æ˜" in point:
                        formatted_text.append(f"  - {point['èª¬æ˜']}")
                        
                # Add detailed analysis
                formatted_text.append("\n### è©³ç´°åˆ†æ")
                for analysis in summary["è©³ç´°åˆ†æ"]:
                    formatted_text.append(f"#### {analysis['ã‚»ã‚¯ã‚·ãƒ§ãƒ³']}")
                    for point in analysis.get("ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ", []):
                        formatted_text.append(f"- {point}")
                        
                # Add keywords
                formatted_text.append("\n### ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
                for keyword in summary["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]:
                    formatted_text.append(f"- **{keyword['ç”¨èª']}**: {keyword['èª¬æ˜']}")
                    
                formatted_text.append("\n---\n")
                
            return "\n".join(formatted_text)
            
        except Exception as e:
            logger.error(f"Error formatting summaries: {str(e)}")
            return "è¦ç´„ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

    def generate_summary(self, text: str) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸè¦ç´„ã‚’ç”Ÿæˆ"""
        try:
            chunks = self._chunk_text(text)
            summaries = []
            previous_summaries = []
            
            for i, chunk in enumerate(chunks):
                context = self._get_chunk_context(previous_summaries)
                
                # Add retry logic for API calls
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = self.model.generate_content(
                            self._create_summary_prompt(chunk, context),
                            generation_config=genai.types.GenerationConfig(
                                temperature=0.3,
                                top_p=0.8,
                                top_k=40,
                                max_output_tokens=8192,
                            )
                        )
                        
                        if not response.text:
                            continue
                            
                        result = self._validate_json_response(response.text)
                        if result:
                            summaries.append(result)
                            previous_summaries.append(result)
                            break
                    except Exception as e:
                        logger.warning(f"Attempt {attempt + 1} failed: {str(e)}")
                        if attempt == max_retries - 1:
                            logger.error(f"All attempts failed for chunk {i}")
            
            if not summaries:
                raise ValueError("No valid summaries generated")
                
            return self._format_summaries(summaries)
            
        except Exception as e:
            logger.error(f"Error in summary generation: {str(e)}")
            raise Exception(f"Failed to generate summary: {str(e)}")

import logging
from youtube_transcript_api import YouTubeTranscriptApi
import google.generativeai as genai
import re
import os
import time
import random
from typing import List, Optional, Dict, Any, Tuple, Callable
from retrying import retry
from cachetools import TTLCache, cached
from concurrent.futures import ThreadPoolExecutor, as_completed

# Enhanced logging setup
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TextProcessor:
    def __init__(self):
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("Gemini API key is not set in environment variables")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-1.5-pro')
        self.safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        # Enhanced caching with separate caches for different operations
        self.subtitle_cache = TTLCache(maxsize=100, ttl=3600)  # 1 hour TTL
        self.summary_cache = TTLCache(maxsize=50, ttl=1800)    # 30 minutes TTL
        self.chunk_cache = TTLCache(maxsize=200, ttl=900)      # 15 minutes TTL
        
        # Rate limiting settings
        self.request_timestamps = []
        self.max_requests_per_minute = 50
        self.min_request_interval = 1.2  # seconds

        # Japanese text normalization patterns
        self.jp_normalization = {
            'brackets': {
                'ã€Œ': '', 'ã€': '', 'ã€': '', 'ã€': '',
                'ï¼ˆ': '', 'ï¼‰': '', 'ã€': '', 'ã€‘': ''
            },
            'punctuation': {
                'ã€': ' ', 'ï¼Œ': ' ',
                'ã€‚': '. ', 'ï¼': '. '
            }
        }
        
        # Initialize noise patterns
        self._init_noise_patterns()

    def _preprocess_text(self, text: str) -> str:
        """Preprocess text with enhanced normalization"""
        # Remove noise patterns
        for pattern in self.noise_patterns.values():
            text = re.sub(pattern, ' ', text)
            
        # Apply Japanese text normalization
        for brackets in self.jp_normalization['brackets'].items():
            text = text.replace(brackets[0], brackets[1])
        for punct in self.jp_normalization['punctuation'].items():
            text = text.replace(punct[0], punct[1])
            
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def _chunk_text(self, text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
        # å…¥åŠ›æ¤œè¨¼ã®å¼·åŒ–
        if not text or len(text.strip()) < 200:
            raise ValueError("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™ï¼ˆæœ€å°200æ–‡å­—å¿…è¦ï¼‰")
        
        # æ–‡ç« ã®å‰å‡¦ç†
        text = self._preprocess_text(text)
        
        # æ–‡å˜ä½ã§ã®åˆ†å‰²
        sentences = re.split('([ã€‚!?ï¼ï¼Ÿ]+)', text)
        sentences = [''.join(i) for i in zip(sentences[0::2], sentences[1::2])]
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®å‹•çš„èª¿æ•´
            if current_length + len(sentence) > chunk_size and current_chunk:
                chunk_text = ' '.join(current_chunk)
                if len(chunk_text) >= 200:
                    chunks.append(chunk_text)
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®å‡¦ç†
                overlap_text = ' '.join(current_chunk[-2:])  # æœ€å¾Œã®2æ–‡ã‚’ä¿æŒ
                current_chunk = [overlap_text] if overlap_text else []
                current_length = len(overlap_text) if overlap_text else 0
            
            current_chunk.append(sentence)
            current_length += len(sentence)
        
        # æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†
        if current_chunk:
            final_chunk = ' '.join(current_chunk)
            if len(final_chunk) >= 200:
                chunks.append(final_chunk)
        
        # ãƒãƒ£ãƒ³ã‚¯ã®æ¤œè¨¼
        if not chunks:
            raise ValueError("æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
        
        return chunks

    def _generate_chunk_summary(self, chunk: str) -> str:
        prompt = f'''
        ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼š
        
        è¦ä»¶ï¼š
        1. é‡è¦ãªæƒ…å ±ã‚’ä¿æŒ
        2. æ–‡è„ˆã‚’ç¶­æŒ
        3. ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾
        4. å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã¨ã®ä¸€è²«æ€§ã‚’è€ƒæ…®
        
        ãƒ†ã‚­ã‚¹ãƒˆï¼š
        {chunk}
        '''
        
        response = self.model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.3,
                top_p=0.8,
                top_k=40,
                max_output_tokens=400
            )
        )
        
        if not response or not response.text:
            raise ValueError("è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        return response.text.strip()

    def _init_noise_patterns(self):
        """Initialize enhanced noise patterns for better text cleaning"""
        self.noise_patterns = {
            'timestamps': r'\[?\(?\d{1,2}:\d{2}(?::\d{2})?\]?\)?',
            'speaker_tags': r'\[[^\]]*\]|\([^)]*\)',
            'filler_words': r'\b(ãˆãƒ¼ã¨|ãˆã£ã¨|ãˆãƒ¼|ã‚ã®|ã‚ã®ãƒ¼|ã¾ã|ã‚“ãƒ¼|ãã®ãƒ¼|ãªã‚“ã‹|ã“ã†|ã­|ã­ã‡|ã•ã|ã†ãƒ¼ã‚“|ã‚ãƒ¼|ãã†ã§ã™ã­|ã¡ã‚‡ã£ã¨)\b',
            'repeated_chars': r'([^\W\d_])\1{3,}',
            'multiple_spaces': r'[\sã€€]{2,}',
            'empty_lines': r'\n\s*\n',
            'punctuation': r'([ã€‚ï¼ï¼ï¼Ÿ])\1+',
            'noise_symbols': r'[â™ªâ™«â™¬â™©â€ â€¡â—Šâ—†â—‡â– â–¡â–²â–³â–¼â–½â—‹â—â—]',
            'unnecessary_symbols': r'[ï¼Šâˆ—â€»#ï¼ƒâ˜…â˜†â–ºâ–·â—â—€â†’â†â†‘â†“]'
        }

    def _enforce_rate_limit(self):
        """Enhanced rate limiting with dynamic adjustment"""
        current_time = time.time()
        self.request_timestamps = [ts for ts in self.request_timestamps 
                                 if current_time - ts < 60]
        
        if len(self.request_timestamps) >= self.max_requests_per_minute:
            sleep_time = 60 - (current_time - self.request_timestamps[0]) + 1
            logger.warning(f"Rate limit reached, waiting {sleep_time:.2f} seconds")
            time.sleep(sleep_time)
        
        time_since_last_request = (current_time - self.request_timestamps[-1] 
                                 if self.request_timestamps else float('inf'))
        if time_since_last_request < self.min_request_interval:
            time.sleep(self.min_request_interval - time_since_last_request)
        
        self.request_timestamps.append(time.time())

    def generate_summary(self, text: str, chunk_size: int = 800) -> str:
        """Generate a summary with improved quality and performance"""
        try:
            # Input validation
            if not text or len(text.strip()) < 200:
                raise ValueError("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™ï¼ˆæœ€å°200æ–‡å­—å¿…è¦ï¼‰")

            # Cache check
            cache_key = hash(text)
            cached_summary = self.summary_cache.get(cache_key)
            if cached_summary:
                logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸè¦ç´„ã‚’ä½¿ç”¨ã—ã¾ã™")
                return cached_summary

            # Process text in chunks
            chunks = self._chunk_text(text, chunk_size=chunk_size)
            summaries = []

            # Process chunks with parallel execution
            with ThreadPoolExecutor(max_workers=3) as executor:
                future_to_chunk = {executor.submit(self._generate_chunk_summary, chunk): i 
                                 for i, chunk in enumerate(chunks)}
                
                for future in as_completed(future_to_chunk):
                    try:
                        chunk_summary = future.result()
                        if chunk_summary:
                            summaries.append((future_to_chunk[future], chunk_summary))
                    except Exception as e:
                        logger.error(f"ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")

            if not summaries:
                raise ValueError("è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

            # Sort summaries by original chunk order
            summaries.sort(key=lambda x: x[0])
            final_text = "\n".join([summary for _, summary in summaries])

            # Generate final summary
            final_summary = self._generate_chunk_summary(final_text)
            if not final_summary:
                raise ValueError("æœ€çµ‚è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

            # Cache the result
            self.summary_cache[cache_key] = final_summary
            return final_summary

        except Exception as e:
            error_msg = f"è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)

    @retry(stop_max_attempt_number=3, wait_fixed=2000)
    def get_transcript(self, url: str) -> str:
        """Get transcript with improved error handling and retries"""
        video_id = self._extract_video_id(url)
        if not video_id:
            raise ValueError("ç„¡åŠ¹ãªYouTube URLã§ã™")
        
        try:
            # Check cache first
            cached_transcript = self.subtitle_cache.get(video_id)
            if cached_transcript:
                logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå­—å¹•ã‚’ä½¿ç”¨ã—ã¾ã™")
                return cached_transcript
            
            transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=['ja', 'ja-JP', 'en'])
            transcript = ' '.join([entry['text'] for entry in transcript_list])
            
            cleaned_transcript = self._preprocess_text(transcript)
            self.subtitle_cache[video_id] = cleaned_transcript
            return cleaned_transcript
            
        except Exception as e:
            error_msg = f"å­—å¹•å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)

    def _extract_video_id(self, url: str) -> Optional[str]:
        """Extract YouTube video ID from URL"""
        patterns = [
            r'(?:v=|\/videos\/|embed\/|youtu.be\/|\/v\/|\/e\/|watch\?v%3D|watch\?feature=player_embedded&v=|%2Fvideos%2F|embed%\u200C\u200B2F|youtu.be%2F|%2Fv%2F)([^#\&\?\n]*)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None

    def _validate_input_text(self, text: str) -> bool:
        """Validate input text length and content"""
        if not text:
            raise ValueError("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")
        
        text = text.strip()
        if len(text) < 100:
            raise ValueError("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™ï¼ˆæœ€å°100æ–‡å­—å¿…è¦ï¼‰")
            
        # Check for meaningful content
        normalized_text = ''.join(text.split())
        if len(normalized_text) < 50:
            raise ValueError("æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆå®Ÿè³ªçš„ãªå†…å®¹ãŒ50æ–‡å­—æœªæº€ï¼‰")
            
        return True

    def _clean_text(self, text: str, progress_callback=None) -> str:
        """Enhanced text cleaning with progress tracking"""
        if not text:
            return ""
        
        try:
            total_steps = len(self.jp_normalization) + len(self.noise_patterns) + 1
            current_step = 0
            
            # Normalize Japanese text
            for category, replacements in self.jp_normalization.items():
                for old, new in replacements.items():
                    text = text.replace(old, new)
                current_step += 1
                if progress_callback:
                    progress_callback(current_step / total_steps, f"æ­£è¦åŒ–å‡¦ç†ä¸­: {category}")
            
            # Remove noise patterns
            for pattern_name, pattern in self.noise_patterns.items():
                text = re.sub(pattern, '', text)
                current_step += 1
                if progress_callback:
                    progress_callback(current_step / total_steps, f"ãƒã‚¤ã‚ºé™¤å»ä¸­: {pattern_name}")
            
            # Improve sentence structure
            text = self._improve_sentence_structure(text)
            current_step += 1
            if progress_callback:
                progress_callback(1.0, "æ–‡ç« æ§‹é€ ã®æœ€é©åŒ–å®Œäº†")
            
            return text
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return text

    def _improve_sentence_structure(self, text: str) -> str:
        """Improve Japanese sentence structure and readability"""
        try:
            # Fix sentence endings
            text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*(?=[^ã€ã€ï¼‰])', r'\1\n', text)
            
            # Improve paragraph breaks
            text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*\n\s*([^ã€Œã€ï¼ˆ])', r'\1\n\n\2', text)
            
            # Fix spacing around Japanese punctuation
            text = re.sub(r'\s+([ã€‚ã€ï¼ï¼Ÿã€ã€ï¼‰])', r'\1', text)
            text = re.sub(r'([ã€Œã€ï¼ˆ])\s+', r'\1', text)
            
            return text.strip()
            
        except Exception as e:
            logger.error(f"æ–‡ç« æ§‹é€ ã®æ”¹å–„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            return text

    def proofread_text(self, text: str, progress_callback=None) -> str:
        """Proofread and enhance text readability with progress tracking"""
        if not text:
            return ""
            
        try:
            if progress_callback:
                progress_callback(0.1, "ğŸ” ãƒ†ã‚­ã‚¹ãƒˆè§£æã‚’é–‹å§‹")
            
            # Initial text cleaning with detailed progress
            cleaning_steps = {
                0.15: "ğŸ“ ãƒ•ã‚£ãƒ©ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å»ä¸­...",
                0.20: "ğŸ”¤ æ–‡å­—ã®æ­£è¦åŒ–ã‚’å®Ÿè¡Œä¸­...",
                0.25: "ğŸ“Š ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å‡¦ç†ä¸­...",
                0.30: "âœ¨ ä¸è¦ãªè¨˜å·ã‚’å‰Šé™¤ä¸­..."
            }
            
            for progress, message in cleaning_steps.items():
                if progress_callback:
                    progress_callback(progress, message)
                time.sleep(0.3)  # Visual feedback
            
            text = self._clean_text(text, lambda p, m: progress_callback(0.3 + p * 0.2, m) if progress_callback else None)
            
            if progress_callback:
                progress_callback(0.5, "ğŸ¤– AIãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ–‡ç« æ ¡æ­£ã‚’æº–å‚™ä¸­...")
            
            # AI Processing with safety settings
            prompt = f"""
            ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¡é–²ã—ã€æ–‡ç« ã‚’æ•´å½¢ã—ã¦ãã ã•ã„ï¼š
            
            {text}
            """
            
            response = self.model.generate_content(
                prompt,
                safety_settings=self.safety_settings,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    top_p=0.8,
                    top_k=40,
                    max_output_tokens=1024,
                )
            )
            
            if not response or not response.text:
                logger.error("AIãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã—ãŸ")
                if progress_callback:
                    progress_callback(1.0, "âŒ ã‚¨ãƒ©ãƒ¼: AIãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å¿œç­”ãŒç©ºã§ã™")
                return text
            
            enhanced_text = response.text.strip()
            enhanced_text = self._clean_text(enhanced_text)
            enhanced_text = self._improve_sentence_structure(enhanced_text)
            
            if progress_callback:
                progress_callback(1.0, "âœ¨ æ ¡æ­£å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!")
            
            return enhanced_text
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆã®æ ¡æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            if progress_callback:
                progress_callback(1.0, f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return text